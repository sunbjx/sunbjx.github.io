<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[CentOS7源码安装mysql5.6(社区版)]]></title>
    <url>%2F2017%2F09%2F25%2FCentOS7%E6%BA%90%E7%A0%81%E5%AE%89%E8%A3%85mysql5-6-%E7%A4%BE%E5%8C%BA%E7%89%88%2F</url>
    <content type="text"><![CDATA[确认安装版本mysql分为开发版本和稳定版本（GA），开发版本拥有最新的特性，但是并不稳定，也没有完全经过测试，可能存在严重的bug，而稳定版本是经过了长时间的测试，消除了具有已知的bug，其稳定性和安全性都得到一定的保障。 对于一个mysql的版本号如：mysql-5.6.1-m1，这个版本号意味着什么呢？ 对于5.6.1的解释：第一个数字5代表了文件格式，第二个数字6代表了发行级别，第三个数字1代表了版本号。更新幅度较小时，最后的数字会增加，出现了重大特性更新时，第二个数字会增加，文件格式改变时，第一个数字会增加 对于m1的解释：这是用来表明这个mysql版本的稳定性级别的，如果没有这个后缀，那么这个版本就是一个稳定版（GA）；如果这个后缀是mN（例如m1，m2）格式，表明了这个版本加入了一些经过彻底测试的新特性，可以认为这是一个试生产的模具；如果这个后缀是rc，表明了这是一个候选版本，已经修改了已知的重要bug，但是没有经过足够长时间的使用来确认所有的bug已经被修复。 一旦选择了版本号，就要选择使用哪个发行版，你可以使用二进制发行版如RPM包或Zip压缩包等，但是如果你要实现如下的功能，就要选择源码安装（本文正是选择源码安装的方式）： 把mysq安装到指定位置 使用mysql的一些特性（标准的二进制版本中并没有这些特性）如：TCP封包支持，调试mysql 二进制版本中默认支持所有的字符集，但你可以在编译安装源码时指定字符集，从而使得安装的mysql更小 卸载原有MySQL或者MariadbCentOs7版本默认情况下安装了mariadb-libs，必须先卸载才可以继续安装MySql 查找以前是否安装mariadb-libs如：mariadb-libs-5.5.35-3.el7.x86_64 1$ rpm -qa | grep -i mariadb-libs 卸载已经安装的mariadb-libs1$ yum remove mariadb-libs-5.5.35-3.el7.x86_64 查找以前是否安装MySQL1$ rpm -qa | grep -i mysql 如果显示有数据 说明已经安装了 MySQL 程序 停止mysql服务 1$ service mysql stop 删除之前安装的mysql12$ rpm -ve 文件名称 例如：MySQL-server-5.6.24-1.linux_glibc2.5.x86_64$ rpm -ve 文件名称 例如：MySQL-client-5.6.24-1.linux_glibc2.5.x86_64 查找之前老版本mysql的目录、并且删除老版本mysql的文件和库1$ find / -name mysql 如： /var/lib/mysql /usr/lib64/mysql /usr/local/mysql /usr/local/mysql/data/mysql 删除对应的目录 123$ rm -rf /var/lib/mysql $ rm -rf /usr/lib64/mysql $ rm -rf /usr/local/mysql 删除配置文档 1$ rm -rf /etc/my.cnf 下载MySQL下载完后需要检查文件的MD5，以确认是否从官网下载的原版本（以防被人篡改过该软件） 我选择如下：官网地址 编译和安装MySQL安装环境12345$ yum -y install make bison-devel ncures-devel libaio$ yum -y install libaio libaio-devel$ yum -y install perl-Data-Dumper$ yum -y install net-tools$ yum -i install bison gcc-c++ cmake ncurses 解压 1$ tar -zxvf mysql-5.6.32.tar.gz 编译安装123$ cmake \-DCMAKE_INSTALL_PREFIX=/usr/local/mysql56 -DMYSQL_DATADIR=/usr/local/mysql/data -DSYSCONFDIR=/etc/my.cnf -DWITH_MYISAM_STORAGE_ENGINE=1 -DWITH_INNOBASE_STORAGE_ENGINE=1 -DWITH_MEMORY_STORAGE_ENGINE=1 -DWITH_READLINE=1 -DMYSQL_UNIX_ADDR=/tmp/mysqld.sock -DMYSQL_TCP_PORT=3306 -DENABLED_LOCAL_INFILE=1 -DWITH_PARTITION_STORAGE_ENGINE=1 -DEXTRA_CHARSETS=all -DDEFAULT_CHARSET=utf8 -DDEFAULT_COLLATION=utf8_general_ci$ make &amp;&amp; make install 123456789101112131415161718192021222324252627# -DCMAKE_INSTALL_PREFIX=/usr/local/mysql56 \ #安装路径 # -DMYSQL_DATADIR=/usr/local/mysql/data \ #数据文件存放位置 # -DSYSCONFDIR=/etc \ #my.cnf路径 # -DWITH_MYISAM_STORAGE_ENGINE=1 \ #支持MyIASM引擎 # -DWITH_INNOBASE_STORAGE_ENGINE=1 \ #支持InnoDB引擎 # -DWITH_MEMORY_STORAGE_ENGINE=1 \ #支持Memory引擎 # -DWITH_READLINE=1 \ #快捷键功能(我没用过) # -DMYSQL_UNIX_ADDR=/tmp/mysqld.sock \ #连接数据库socket路径 # -DMYSQL_TCP_PORT=3306 \ #端口 # -DENABLED_LOCAL_INFILE=1 \ #允许从本地导入数据 # -DWITH_PARTITION_STORAGE_ENGINE=1 \ #安装支持数据库分区 # -DEXTRA_CHARSETS=all \ #安装所有的字符集 # -DDEFAULT_CHARSET=utf8 \ #默认字符 # -DDEFAULT_COLLATION=utf8_general_ci 配置MySQL 检查系统是否已经有mysql用户，如果没有则创建 12$ cat /etc/passwd | grep mysql$ cat /etc/group | grep mysql 创建mysql用户（但是不能使用mysql账号登陆系统 12$ groupadd mysql$ useradd -g -s mysql mysql 修改权限 123456789$ chown -R mysql:mysql /usr/local/mysql$ cd /usr/local/mysql$ chown -R mysql:mysql .$ scripts/mysql_install_db --user=mysql# 将权限设置给root用户，并设置给mysql组， 取消其他用户的读写执行权限，# 仅留给mysql &quot;rx&quot;读执行权限，其他用户无任何权限$ chown -R root:mysql .$ chown -R mysql:mysql ./data$ chmod -R ug+rwx . 将mysql的配置文件拷贝到/etc 123$ cp support-files/my-default.cnf /etc/my.cnf# 注意：5.6 之前如下$ cp support-files/my-medium.cnf /etc/my.cnf 修改my.cnf配置 1$ vim /etc/my.cnf [mysqld] 下面添加： user=mysql datadir=/data/mysql default-storage-engine=MyISAM 启动mysql 123# 将mysql的启动服务添加到系统服务中$ cp support-files/mysql.server /etc/init.d/mysql$ service mysql start 增加MySQL服务 1$ chkconfig --add mysql 修改root用户密码 123$ cd /usr/local/mysql56$ ./bin/mysqladmin -u root password$ service mysql restart 常用基础操作12345678910111213$ mysql -u root -p root-- 查看数据库show databases;-- 创建用户并允许本地用户通过密码登录create user &apos;username&apos;@&apos;localhost&apos; identified by &apos;user-password&apos;;-- 创建数据库create database schema_name;-- 不可授权给其他用户(* 表示该数据库下所有的表名)grant all privileges on schema_name.* to &apos;username&apos;@&apos;remote-ip&apos; identified by &apos;remote-user-password&apos; -- 可授权给其他用户grant all privileges on schema_name.* to &apos;username&apos;@&apos;remote-ip&apos; identified by &apos;remote-user-password&apos; with grant option;-- 刷新flush privileges;]]></content>
      <categories>
        <category>DB</category>
        <category>MySQL</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Centos7下RabbitMQ服务安装配置]]></title>
    <url>%2F2017%2F09%2F22%2FCentos7%E4%B8%8BRabbitMQ%E6%9C%8D%E5%8A%A1%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[安装 Erlang下载源码(官网地址:)1$ wget http://erlang.org/download/otp_src_19.3.tar.gz 安装依赖1$ yum install gcc glibc-devel make ncurses-devel openssl-devel xmlto 解压第一步下载的源码12$ tar zxvf otp_src_19.3.tar.gz$ cd opt_src_19.3 配置安装路径编译代码1$ ./configure --prefix=/opt/erlang 执行编译结果1$ make &amp;&amp; make install 验证1$ ./opt/erlang/bin/erl 设置环境变量vim /etc/profile12#set erlang environmentexport PATH=$PATH:/opt/erlang/bin 编译环境变量1$ source /etc/profile 安装RabbitMQ下载(官网地址:)1$ wget http://www.rabbitmq.com/releases/rabbitmq-server/v3.6.11/rabbitmq-server-generic-unix-3.6.11.tar.xz 解压文件并重命名1234$ xz -d rabbitmq-server-generic-unix-3.6.1.tar.xz$ tar -xvf rabbitmq-server-generic-unix-3.6.1.tar -C /opt$ cd /opt$ mv rabbitmq_server-3.6.11 rabbitmq 设置环境变量vim /etc/profile12#set rabbitmq environmentexport PATH=$PATH:/opt/rabbitmq/sbin 编译环境变量1$ source /etc/profile 配置网页插件12$ mkdir /etc/rabbitmq$ ./rabbitmq-plugins enable rabbitmq_management 远程访问配置 添加用户:rabbitmqctl add_user hxb hxb 添加权限:rabbitmqctl set_permissions -p &quot;/&quot; hxb &quot;.*&quot; &quot;.*&quot; &quot;.*&quot; 修改用户角色:rabbitmqctl set_user_tags hxb administrator RabbitMQ常用命令add_user &lt;UserName&gt; &lt;Password&gt; delete_user &lt;UserName&gt; change_password &lt;UserName&gt; &lt;NewPassword&gt; list_users add_vhost &lt;VHostPath&gt; delete_vhost &lt;VHostPath&gt; list_vhostsset_permissions [-p &lt;VHostPath&gt;] &lt;UserName&gt; &lt;Regexp&gt; &lt;Regexp&gt; &lt;Regexp&gt; clear_permissions [-p &lt;VHostPath&gt;] &lt;UserName&gt; list_permissions [-p &lt;VHostPath&gt;] list_user_permissions &lt;UserName&gt; list_queues [-p &lt;VHostPath&gt;] [&lt;QueueInfoItem&gt; ...] list_exchanges [-p &lt;VHostPath&gt;] [&lt;ExchangeInfoItem&gt; ...] list_bindings [-p &lt;VHostPath&gt;] list_connections [&lt;ConnectionInfoItem&gt; ...]]]></content>
      <categories>
        <category>middleware</category>
        <category>message</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[我让自己自由了三年]]></title>
    <url>%2F2017%2F09%2F18%2F%E6%88%91%E8%AE%A9%E8%87%AA%E5%B7%B1%E8%87%AA%E7%94%B1%E4%BA%86%E4%B8%89%E5%B9%B4%2F</url>
    <content type="text"><![CDATA[我让自己自由了三年-迟来的人生规划（未来的3年计划5年规划） 感觉这一刻才刚开始（毕业后的面试中总会遇到你未来的五年规划或者计划是什么？支支吾吾了下-😅 接着问到你未来三年是什么或者接下来一年是什么？接着😅） 就这样尴尬了三年（工作上一直在被动工作，学习上几乎没有，最后经历了由恋爱到寂寞与孤独） 2018-2020年 我的人生第一个三年计划2021-2025年 我的人生第一个五年规划2018-2020年 我要在成都在英孚把我的英语培训起来（至少要养成一个每天学英语的好习惯）；工作上我必须在devops这一块不断的夯实自己 加倍时间去完成（主打java 涉及python go js）；生活里能否找到自己心仪的适合结婚的好女孩😊；在父母的帮助下必须一定得自己去在成都按揭一套心仪的房子；车能不买就不买 买了也是吃灰😂🤦‍♂️2021-2025年 在一个自己喜欢的公司把它当成自己的公司尽我所能看看我能为公司创造多大的效益并且对比一下公司为我的花费（公司地址不局限于国内😄💪） 这时候自己35岁了必须得有一个幸福的家庭（在此之前能够兼顾到并完成当然更好😂💪）；选择与能够非常非常非常信任的朋友大家一条心的去创业（产品方向能够为社会为人们带来便利与好处）or 能遇到一个理想的好平台达到和自己创业一致的能够高速高质量稳定的驶向远方 让自己对这个世界甚至这茫茫星云中了解的更多探索的更多 这一切都必须得身体健康⛽️ 大叔阶段能把妹，花甲之年能抱孙子孙女，有生之年能见重孙重孙女 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;—程序员也有的幸福]]></content>
      <categories>
        <category>规划与计划</category>
        <category>人生规划</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[在CentOS7上部署kubernetes1.7 HA 集群(Vagrant)]]></title>
    <url>%2F2017%2F09%2F13%2F%E5%9C%A8CentOS7%E4%B8%8A%E9%83%A8%E7%BD%B2kubernetes1-7-HA-%E9%9B%86%E7%BE%A4-Vagrant%2F</url>
    <content type="text"><![CDATA[使用virtualbox + vagrant安装centos7vagrant准本工作1、具体安装和使用virtualbox &amp;&amp; vagrant请自行Google 2、搜索centos7 box:https://app.vagrantup.com/boxes/search?provider=virtualbox 3、生成Vagrantfile 123mkdir -p ~/Vagrant/k8s-clustercd ~/Vagrant/k8s-clustervagrant init centos/7 安装centos71、编辑Vagrantfile 123cd ~/Vagrant/k8s-clustercp Vagrantfile ./Vagrantfile_backupemacs Vagrantfile Vagrantfile 内容如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081# coding: utf-8Vagrant.configure(&quot;2&quot;) do |config| (11..13).each do |i| config.vm.define &quot;k8s-master#&#123;i&#125;&quot; do |node| # 设置虚拟机的Box node.vm.box = &quot;centos/7&quot; # 设置虚拟机的主机名 node.vm.hostname=&quot;master#&#123;i&#125;&quot; # 设置虚拟机的IP node.vm.network &quot;public_network&quot;, ip: &quot;192.168.7.#&#123;i&#125;&quot;, :bridge=&gt;&apos;en0: Wi-Fi (AirPort)&apos; node.vm.boot_timeout = 20 # 设置主机与虚拟机的共享目录 node.vm.synced_folder &quot;/Users/sunbjx/Vagrant/share&quot;, &quot;/home/vagrant/share&quot; # VirtaulBox相关配置 node.vm.provider &quot;virtualbox&quot; do |v| # 设置虚拟机的名称 v.name = &quot;k8s-master#&#123;i&#125;&quot; # 设置虚拟机的内存大小 v.memory = 4096 # 设置虚拟机的CPU个数 v.cpus = 2 end # 使用shell脚本进行软件安装和配置 node.vm.provision &quot;shell&quot;, inline: &lt;&lt;-SHELL sudo yum -y update sudo yum -y install net-tools sudo yum -y install vim SHELL end end (14..15).each do |i| config.vm.define &quot;k8s-node#&#123;i&#125;&quot; do |node| # 设置虚拟机的Box node.vm.box = &quot;centos/7&quot; # 设置虚拟机的主机名 node.vm.hostname=&quot;node#&#123;i&#125;&quot; # 设置虚拟机的IP node.vm.network &quot;public_network&quot;, ip: &quot;192.168.7.#&#123;i&#125;&quot;, :bridge=&gt;&apos;en0: Wi-Fi (AirPort)&apos; node.vm.boot_timeout = 20 # 设置主机与虚拟机的共享目录 node.vm.synced_folder &quot;/Users/sunbjx/Vagrant/share&quot;, &quot;/home/vagrant/share&quot; # VirtaulBox相关配置 node.vm.provider &quot;virtualbox&quot; do |v| # 设置虚拟机的名称 v.name = &quot;k8s-node#&#123;i&#125;&quot; # 设置虚拟机的内存大小 v.memory = 8192 # 设置虚拟机的CPU个数 v.cpus = 2 end # 使用shell脚本进行软件安装和配置 node.vm.provision &quot;shell&quot;, inline: &lt;&lt;-SHELL sudo yum -y update sudo yum -y install net-tools sudo yum -y install vim SHELL end endend 2、启动安装centos7 1vagrant up 环境说明环境总共 5 台虚拟机，3 个 master，3 个 etcd 节点，2 个 node 网络方案这里采用flannel，集群开启 RBAC IP 节点 192.168.7.11 master、etcd 192.168.7.12 master、etcd 192.168.7.13 master、etcd 192.168.7.14 node 192.168.7.15 node 创建 TLS证书和秘钥证书说明 证书名称 配置文件 用途 etcd-root-ca.pem etcd-root-ca-csr.json etcd 根 CA 证书 etcd.pem etcd-gencert.json、etcd-csr.json etcd 集群证书 k8s-root-ca.pem k8s-root-ca-csr.json k8s 根 CA 证书 kube-proxy.pem k8s-gencert.json、kube-proxy-csr.json kube-proxy 使用的证书 admin.pem k8s-gencert.json、admin-csr.json kubectl 使用的证书 kubernetes.pem k8s-gencert.json、kubernetes-csr.json kube-apiserver 使用的证书 安装CFSSL (192.168.7.11)12345678910111213mkdir -p ~/downloadcd ~/download# 下载 go1.8.3wget https://golang.org/dl/go1.8.3.linux-amd64.tar.gz# 解压压缩包tar -C /usr/local -xzf go1.8.3.linux-amd64.tar.gz# 添加环境变量vim /etc/profileexport GOROOT=/usr/local/goexport PATH=$PATH:$GOROOT/binsource /etc/profile# 使用 go 命令安装 cfsslgo get -u github.com/cloudflare/cfssl/cmd/... 生成etcd证书 (192.168.7.11) etcd 证书配置文件 12mkdir -p ~/document/sslcd ~/document/ssl vim etcd-root-ca-csr.json 12345678910111213141516&#123; &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 4096 &#125;, &quot;names&quot;: [ &#123; &quot;O&quot;: &quot;etcd&quot;, &quot;OU&quot;: &quot;etcd Security&quot;, &quot;L&quot;: &quot;Beijing&quot;, &quot;ST&quot;: &quot;Beijing&quot;, &quot;C&quot;: &quot;CN&quot; &#125; ], &quot;CN&quot;: &quot;etcd-root-ca&quot;&#125; vim etcd-gencert.json 12345678910111213&#123; &quot;signing&quot;: &#123; &quot;default&quot;: &#123; &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot; ], &quot;expiry&quot;: &quot;87600h&quot; &#125; &#125;&#125; vim etcd-csr.json 12345678910111213141516171819202122232425&#123; &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 4096 &#125;, &quot;names&quot;: [ &#123; &quot;O&quot;: &quot;etcd&quot;, &quot;OU&quot;: &quot;etcd Security&quot;, &quot;L&quot;: &quot;Beijing&quot;, &quot;ST&quot;: &quot;Beijing&quot;, &quot;C&quot;: &quot;CN&quot; &#125; ], &quot;CN&quot;: &quot;etcd&quot;, &quot;hosts&quot;: [ &quot;127.0.0.1&quot;, &quot;localhost&quot;, &quot;192.168.1.11&quot;, &quot;192.168.1.12&quot;, &quot;192.168.1.13&quot;, &quot;192.168.1.14&quot;, &quot;192.168.1.15&quot; ]&#125; “CN”：Common Name，kube-apiserver 从证书中提取该字段作为请求的用户名 (User Name)；浏览器使用该字段验证网站是否合法；“O”：Organization，kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group) 生成 etcd 证书 12cfssl gencert --initca=true etcd-root-ca-csr.json | cfssljson --bare etcd-root-cacfssl gencert --ca etcd-root-ca.pem --ca-key etcd-root-ca-key.pem --config etcd-gencert.json etcd-csr.json | cfssljson --bare etcd 生成 kubernetes 证书 (192.168.7.11) kubernetes 证书配置文件 vim k8s-root-ca-csr.json 12345678910111213141516&#123; &quot;CN&quot;: &quot;kubernetes&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 4096 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;BeiJing&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125; vim k8s-gencert.json 123456789101112131415161718&#123; &quot;signing&quot;: &#123; &quot;default&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot; &#125;, &quot;profiles&quot;: &#123; &quot;kubernetes&quot;: &#123; &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot; ], &quot;expiry&quot;: &quot;87600h&quot; &#125; &#125; &#125;&#125; vim kubernetes-csr.json 12345678910111213141516171819202122232425262728293031&#123; &quot;CN&quot;: &quot;kubernetes&quot;, &quot;hosts&quot;: [ &quot;127.0.0.1&quot;, &quot;10.254.0.1&quot;, &quot;192.168.7.11&quot;, &quot;192.168.7.12&quot;, &quot;192.168.7.13&quot;, &quot;192.168.7.14&quot;, &quot;192.168.7.15&quot;, &quot;localhost&quot;, &quot;kubernetes&quot;, &quot;kubernetes.default&quot;, &quot;kubernetes.default.svc&quot;, &quot;kubernetes.default.svc.cluster&quot;, &quot;kubernetes.default.svc.cluster.local&quot; ], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;BeiJing&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125; vim kube-proxy-csr.json 1234567891011121314151617&#123; &quot;CN&quot;: &quot;system:kube-proxy&quot;, &quot;hosts&quot;: [], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;BeiJing&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125; vim admin-csr.json 1234567891011121314151617&#123; &quot;CN&quot;: &quot;admin&quot;, &quot;hosts&quot;: [], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;BeiJing&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;O&quot;: &quot;system:masters&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125; 生成 kubernetes 证书 12345cfssl gencert --initca=true k8s-root-ca-csr.json | cfssljson --bare k8s-root-cafor targetName in kubernetes admin kube-proxy; do cfssl gencert --ca k8s-root-ca.pem --ca-key k8s-root-ca-key.pem --config k8s-gencert.json --profile kubernetes $targetName-csr.json | cfssljson --bare $targetNamedone 分发所需证书到各个节点目录 /etc/kubernetes/ssl 下面 123456789for IP in `seq 1 5`;do ssh root@192.168.1.1$IP mkdir -p /etc/etcd/ssl sudo scp ~/document/ssl/etcd*.pem root@192.168.1.1$IP:/etc/etcd/ssl ssh root@192.168.1.1$IP chown -R etcd:etcd /etc/etcd/ssl ssh root@192.168.1.1$IP chmod -R 755 /etc/etcd ssh root@192.168.7.1$IP mkdir -p /etc/kubernetes/ssl sudo scp ~/document/ssl/*.pem root@192.168.7.1$IP:/etc/kubernetes/ssl ssh root@192.168.7.1$IP chown -R kube:kube /etc/kubernetes/ssldone 生成 token 及 kubeconfig (192.168.7.11)kubelet、kube-proxy 等 Node 机器上的进程与 Master 机器的 kube-apiserver 进程通信时需要认证和授权；kubernetes 1.4 开始支持由 kube-apiserver 为客户端生成 TLS 证书的 TLS Bootstrapping 功能，这样就不需要为每个客户端生成证书了；该功能当前仅支持为 kubelet 生成证书 生成 token 12345cd /etc/kubernetesexport BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d &apos; &apos;)cat &gt; token.csv &lt;&lt;EOF$&#123;BOOTSTRAP_TOKEN&#125;,kubelet-bootstrap,10001,&quot;system:kubelet-bootstrap&quot;EOF BOOTSTRAP_TOKEN 将被写入到 kube-apiserver 使用的 token.csv 文件和 kubelet 使用的 bootstrap.kubeconfig 文件，如果后续重新生成了 BOOTSTRAP_TOKEN，则需要：更新 token.csv 文件，分发到所有机器 (master 和 node）的 /etc/kubernetes/ 目录下，分发到node节点上非必需；重新生成 bootstrap.kubeconfig 文件，分发到所有 node 机器的 /etc/kubernetes/ 目录下；重启 kube-apiserver 和 kubelet 进程；重新 approve kubelet 的 csr 请求 安装 kubernetes 并将 kube-apiserver,kube-controller-manager,kube-scheduler,kubectl 分发到 master; kubelet,kube-proxy 分发到 node; 同时分发所需证书到各个节点目录 /etc/kubernetes/ssl 下面 123456789101112131415161718cd ~/downloadwget https://github.com/kubernetes/kubernetes/releases/download/v1.7.4/kubernetes.tar.gztar -zxvf kubernetes.tar.gzcd kubernetes./cluster/get-kube-binaries.shcd kubernetes/servertar -zxvf kubernetes-server-linux-amd64.tar.gzcd kubernetes/server/binfor IP in `seq 1 3`;do sudo scp ./&#123;kube-apiserver,kube-controller-manager,kube-scheduler,kubectl&#125; root@192.168.7.1$IP:/usr/local/bin/ ssh root@192.168.7.1$IP chmod a+x /usr/local/bin/kube*donefor IP in `seq 4 5`;do sudo scp ./&#123;kubelet,kube-proxy&#125; root@192.168.7.1$IP:/usr/local/bin ssh root@192.168.7.1$IP chmod a+x /usr/local/bin/kube*done 创建 kubelet bootstrapping kubeconfig 文件 12345678910111213141516171819cd /etc/kubernetesexport KUBE_APISERVER=&quot;https://192.168.7.11:6443&quot;# 设置集群参数kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/ssl/k8s-root-ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=bootstrap.kubeconfig# 设置客户端认证参数kubectl config set-credentials kubelet-bootstrap \ --token=$&#123;BOOTSTRAP_TOKEN&#125; \ --kubeconfig=bootstrap.kubeconfig# 设置上下文参数kubectl config set-context default \ --cluster=kubernetes \ --user=kubelet-bootstrap \ --kubeconfig=bootstrap.kubeconfig# 设置默认上下文kubectl config use-context default --kubeconfig=bootstrap.kubeconfig –embed-certs 为 true 时表示将 certificate-authority 证书写入到生成的 bootstrap.kubeconfig 文件中；设置客户端认证参数时没有指定秘钥和证书，后续由 kube-apiserver 自动生成 创建 kube-proxy kubeconfig 文件 12345678910111213141516171819# 设置集群参数kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/ssl/k8s-root-ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=kube-proxy.kubeconfig# 设置客户端认证参数kubectl config set-credentials kube-proxy \ --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \ --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \ --embed-certs=true \ --kubeconfig=kube-proxy.kubeconfig# 设置上下文参数kubectl config set-context default \ --cluster=kubernetes \ --user=kube-proxy \ --kubeconfig=kube-proxy.kubeconfig# 设置默认上下文kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig 设置集群参数和客户端认证参数时 –embed-certs 都为 true，这会将 certificate-authority、client-certificate 和 client-key 指向的证书文件内容写入到生成的 kube-proxy.kubeconfig 文件中；kube-proxy.pem 证书中 CN 为 system:kube-proxy，kube-apiserver 预定义的 RoleBinding cluster-admin 将User system:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限 分发 token、kubeconfig 文件 1234for IP in `seq 2 5`;do scp *.kubeconfig root@192.168.7.1$IP:/etc/kubernetes scp token.csv root@192.168.7.1$IP:/etc/kubernetesdone 部署 HA etcd安装 etcd123sudo yum -y install etcdecho &apos;export ETCDCTL_API=3&apos; &gt;&gt; /etc/profilesource /etc/profile 修改配置 vim /etc/etcd/etc.conf123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# [member]ETCD_NAME=etcd1ETCD_DATA_DIR=&quot;/var/lib/etcd/etcd1.etcd&quot;ETCD_WAL_DIR=&quot;/var/lib/etcd/wal&quot;ETCD_SNAPSHOT_COUNT=&quot;100&quot;ETCD_HEARTBEAT_INTERVAL=&quot;100&quot;ETCD_ELECTION_TIMEOUT=&quot;1000&quot;ETCD_LISTEN_PEER_URLS=&quot;https://192.168.7.11:2380&quot;ETCD_LISTEN_CLIENT_URLS=&quot;https://192.168.7.11:2379,http://127.0.0.1:2379&quot;ETCD_MAX_SNAPSHOTS=&quot;5&quot;ETCD_MAX_WALS=&quot;5&quot;#ETCD_CORS=&quot;&quot;##[cluster]ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://192.168.7.11:2380&quot;# if you use different ETCD_NAME (e.g. test), set ETCD_INITIAL_CLUSTER value for this name, i.e. &quot;test=http://...&quot;ETCD_INITIAL_CLUSTER=&quot;etcd1=https://192.168.7.11:2380,etcd2=https://192.168.7.12:2380,etcd3=https://192.168.7.13:2380&quot;ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;ETCD_ADVERTISE_CLIENT_URLS=&quot;https://192.168.7.11:2379&quot;#ETCD_DISCOVERY=&quot;&quot;#ETCD_DISCOVERY_SRV=&quot;&quot;#ETCD_DISCOVERY_FALLBACK=&quot;proxy&quot;#ETCD_DISCOVERY_PROXY=&quot;&quot;#ETCD_STRICT_RECONFIG_CHECK=&quot;false&quot;#ETCD_AUTO_COMPACTION_RETENTION=&quot;0&quot;##[proxy]#ETCD_PROXY=&quot;off&quot;#ETCD_PROXY_FAILURE_WAIT=&quot;5000&quot;#ETCD_PROXY_REFRESH_INTERVAL=&quot;30000&quot;#ETCD_PROXY_DIAL_TIMEOUT=&quot;1000&quot;#ETCD_PROXY_WRITE_TIMEOUT=&quot;5000&quot;#ETCD_PROXY_READ_TIMEOUT=&quot;0&quot;##[security]ETCD_CERT_FILE=&quot;/etc/etcd/ssl/etcd.pem&quot;ETCD_KEY_FILE=&quot;/etc/etcd/ssl/etcd-key.pem&quot;ETCD_CLIENT_CERT_AUTH=&quot;true&quot;ETCD_TRUSTED_CA_FILE=&quot;/etc/etcd/ssl/etcd-root-ca.pem&quot;ETCD_AUTO_TLS=&quot;true&quot;ETCD_PEER_CERT_FILE=&quot;/etc/etcd/ssl/etcd.pem&quot;ETCD_PEER_KEY_FILE=&quot;/etc/etcd/ssl/etcd-key.pem&quot;ETCD_PEER_CLIENT_CERT_AUTH=&quot;true&quot;ETCD_PEER_TRUSTED_CA_FILE=&quot;/etc/etcd/ssl/etcd-root-ca.pem&quot;ETCD_PEER_AUTO_TLS=&quot;true&quot;##[logging]#ETCD_DEBUG=&quot;false&quot;# examples for -log-package-levels etcdserver=WARNING,security=DEBUG#ETCD_LOG_PACKAGE_LEVELS=&quot;&quot; 启动及验证 配置修改后在每个节点进行启动即可，注意，Etcd 各个节点间必须保证时钟同步，否则会造成启动失败等错误 1234567yum -y install ntpsystemctl start ntpdsystemctl enable ntpdhwclock --systohcsystemctl daemon-reloadsystemctl start etcdsystemctl enable etcd 验证 etcd 集群节点状态 12export ETCDCTL_API=3etcdctl --cacert=/etc/etcd/ssl/etcd-root-ca.pem --cert=/etc/etcd/ssl/etcd.pem --key=/etc/etcd/ssl/etcd-key.pem --endpoints=https://192.168.7.11:2379,https://192.168.7.12:2379,https://192.168.7.13:2379 endpoint health 部署 HA master由于 api server 会写入一些日志，所以先创建好相关目录，并做好授权，防止因为权限错误导致 api server 无法启动 12345for IP in `seq 1 3`;do ssh root@192.168.1.1$IP mkdir /var/log/kube-audit ssh root@192.168.1.1$IP chown -R kube:kube /var/log/kube-audit ssh root@192.168.1.1$IP chmod -R 755 /var/log/kube-auditdone 创建 kube-apiserver的service配置文件vim /usr/lib/systemd/system/kube-apiserver.service 1234567891011121314151617181920212223242526[Unit]Description=Kubernetes API ServiceDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.targetAfter=etcd.service[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/apiserverExecStart=/usr/local/bin/kube-apiserver \ $KUBE_LOGTOSTDERR \ $KUBE_LOG_LEVEL \ $KUBE_ETCD_SERVERS \ $KUBE_API_ADDRESS \ $KUBE_API_PORT \ $KUBELET_PORT \ $KUBE_ALLOW_PRIV \ $KUBE_SERVICE_ADDRESSES \ $KUBE_ADMISSION_CONTROL \ $KUBE_API_ARGSRestart=on-failureType=notifyLimitNOFILE=65536[Install]WantedBy=multi-user.target 创建 kube-controller-manager的serivce配置文件vim /usr/lib/systemd/system/kube-controller-manager.service 12345678910111213141516Description=Kubernetes Controller ManagerDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/controller-managerExecStart=/usr/local/bin/kube-controller-manager \ $KUBE_LOGTOSTDERR \ $KUBE_LOG_LEVEL \ $KUBE_MASTER \ $KUBE_CONTROLLER_MANAGER_ARGSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target 创建 kube-scheduler的serivce配置文件vim /usr/lib/systemd/system/kube-scheduler.service 1234567891011121314151617[Unit]Description=Kubernetes Scheduler PluginDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/schedulerExecStart=/usr/local/bin/kube-scheduler \ $KUBE_LOGTOSTDERR \ $KUBE_LOG_LEVEL \ $KUBE_MASTER \ $KUBE_SCHEDULER_ARGSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target 修改 master 配置master 需要编辑 config、apiserver、controller-manager、scheduler这四个文件，具体修改如下 config 通用配置vim /etc/kubernetes/config 12345678910111213141516171819202122#### kubernetes system config## The following values are used to configure various aspects of all# kubernetes services, including## kube-apiserver.service# kube-controller-manager.service# kube-scheduler.service# kubelet.service# kube-proxy.service# logging to stderr means we get it in the systemd journalKUBE_LOGTOSTDERR=&quot;--logtostderr=true&quot;# journal message level, 0 is debugKUBE_LOG_LEVEL=&quot;--v=2&quot;# Should this cluster be allowed to run privileged docker containersKUBE_ALLOW_PRIV=&quot;--allow-privileged=true&quot;# How the controller-manager, scheduler, and proxy find the apiserverKUBE_MASTER=&quot;--master=http://127.0.0.1:8080&quot; apiserver 配置(其他节点只有 IP 不同) vim /etc/kubernetes/apiserver 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#### kubernetes system config## The following values are used to configure the kube-apiserver## The address on the local server to listen to.KUBE_API_ADDRESS=&quot;--advertise-address=192.168.7.11 --insecure-bind-address=127.0.0.1 --bind-address=192.168.7.11&quot;# The port on the local server to listen on.KUBE_API_PORT=&quot;--insecure-port=8080 --secure-port=6443&quot;# Port minions listen on# KUBELET_PORT=&quot;--kubelet-port=10250&quot;# Comma separated list of nodes in the etcd clusterKUBE_ETCD_SERVERS=&quot;--etcd-servers=https://192.168.7.11:2379,https://192.168.7.12:2379,https://192.168.7.13:2379&quot;# Address range to use for servicesKUBE_SERVICE_ADDRESSES=&quot;--service-cluster-ip-range=10.254.0.0/16&quot;# default admission control policiesKUBE_ADMISSION_CONTROL=&quot;--admission-control=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota&quot;# Add your own!KUBE_API_ARGS=&quot;--authorization-mode=RBAC \ --runtime-config=rbac.authorization.k8s.io/v1beta1 \ --anonymous-auth=false \ --kubelet-https=true \ --experimental-bootstrap-token-auth \ --token-auth-file=/etc/kubernetes/token.csv \ --service-node-port-range=30000-50000 \ --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \ --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \ --client-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \ --service-account-key-file=/etc/kubernetes/ssl/k8s-root-ca.pem \ --etcd-quorum-read=true \ --storage-backend=etcd3 \ --etcd-cafile=/etc/etcd/ssl/etcd-root-ca.pem \ --etcd-certfile=/etc/etcd/ssl/etcd.pem \ --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \ --enable-swagger-ui=true \ --apiserver-count=3 \ --audit-log-maxage=30 \ --audit-log-maxbackup=3 \ --audit-log-maxsize=100 \ --audit-log-path=/var/log/kube-audit/audit.log \ --event-ttl=1h&quot; controller-manager 配置 vim /etc/kubernetes/controller-manager 1234567891011121314151617#### The following values are used to configure the kubernetes controller-manager# defaults from config and apiserver should be adequate# Add your own!KUBE_CONTROLLER_MANAGER_ARGS=&quot;--address=127.0.0.1 \ --service-cluster-ip-range=10.254.0.0/16 \ --cluster-name=kubernetes \ --cluster-signing-cert-file=/etc/kubernetes/ssl/k8s-root-ca.pem \ --cluster-signing-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \ --service-account-private-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \ --root-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \ --leader-elect=true \ --node-monitor-grace-period=40s \ --node-monitor-period=5s \ --pod-eviction-timeout=5m0s&quot; scheduler 配置 vim /etc/kubernetes/scheduler 1234567#### kubernetes scheduler config# default config should be adequate# Add your own!KUBE_SCHEDULER_ARGS=&quot;--leader-elect=true --address=127.0.0.1&quot; 其他 master 节点配置相同，只需要修改以下 IP 地址即可 启动各 master 节点相关服务1234567systemctl daemon-reloadsystemctl start kube-apiserversystemctl start kube-controller-managersystemctl start kube-schedulersystemctl enable kube-apiserversystemctl enable kube-controller-managersystemctl enable kube-scheduler 验证 master 节点功能 部署 node创建 ClusterRoleBindingkubelet 启动时向 kube-apiserver 发送 TLS bootstrapping 请求，需要先将 bootstrap token 文件中的 kubelet-bootstrap 用户赋予 system:node-bootstrapper cluster 角色(role)， 然后 kubelet 才能有权限创建认证请求(certificate signing requests)： 1234# 在任意 master 执行即可kubectl create clusterrolebinding kubelet-bootstrap \ --clusterrole=system:node-bootstrapper \ --user=kubelet-bootstrap 创建 kubelet 的service配置文件/usr/lib/systemd/system/kubelet.service 123456789101112131415161718192021222324[Unit]Description=Kubernetes Kubelet ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=docker.serviceRequires=docker.service[Service]WorkingDirectory=/var/lib/kubeletEnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/kubeletExecStart=/usr/local/bin/kubelet \ $KUBE_LOGTOSTDERR \ $KUBE_LOG_LEVEL \ $KUBELET_API_SERVER \ $KUBELET_ADDRESS \ $KUBELET_PORT \ $KUBELET_HOSTNAME \ $KUBE_ALLOW_PRIV \ $KUBELET_POD_INFRA_CONTAINER \ $KUBELET_ARGSRestart=on-failure[Install]WantedBy=multi-user.target 创建 kube-proxy 的service配置文件/usr/lib/systemd/system/kube-proxy.service 123456789101112131415161718[Unit]Description=Kubernetes Kube-Proxy ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.target[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/proxyExecStart=/usr/local/bin/kube-proxy \ $KUBE_LOGTOSTDERR \ $KUBE_LOG_LEVEL \ $KUBE_MASTER \ $KUBE_PROXY_ARGSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target config 通用配置 12345678910111213141516171819202122#### kubernetes system config## The following values are used to configure various aspects of all# kubernetes services, including## kube-apiserver.service# kube-controller-manager.service# kube-scheduler.service# kubelet.service# kube-proxy.service# logging to stderr means we get it in the systemd journalKUBE_LOGTOSTDERR=&quot;--logtostderr=true&quot;# journal message level, 0 is debugKUBE_LOG_LEVEL=&quot;--v=2&quot;# Should this cluster be allowed to run privileged docker containersKUBE_ALLOW_PRIV=&quot;--allow-privileged=true&quot;# How the controller-manager, scheduler, and proxy find the apiserver# KUBE_MASTER=&quot;--master=http://127.0.0.1:8080&quot; kubelet 配置 123456789101112131415161718192021222324252627#### kubernetes kubelet (minion) config# The address for the info server to serve on (set to 0.0.0.0 or &quot;&quot; for all interfaces)KUBELET_ADDRESS=&quot;--address=192.168.7.14&quot;# The port for the info server to serve on# KUBELET_PORT=&quot;--port=10250&quot;# You may leave this blank to use the actual hostnameKUBELET_HOSTNAME=&quot;--hostname-override=docker4.node&quot;# location of the api-server#KUBELET_API_SERVER=&quot;--api-servers=http://192.168.7.11:8080&quot;# Add your own!KUBELET_ARGS=&quot;--cgroup-driver=cgroupfs \ --cluster-dns=10.254.0.2 \ --resolv-conf=/etc/resolv.conf \ --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \ --require-kubeconfig \ --cert-dir=/etc/kubernetes/ssl \ --cluster-domain=cluster.local \ --hairpin-mode promiscuous-bridge \ --serialize-image-pulls=false \ --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.0&quot; proxy 配置 12345678910#### kubernetes proxy config# default config should be adequate# Add your own!KUBE_PROXY_ARGS=&quot;--bind-address=192.168.7.14 \ --hostname-override=docker4.node \ --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \ --cluster-cidr=10.254.0.0/16&quot; 创建 nginx 代理根据上面描述的 master HA 架构，此时所有 node 应该连接本地的 nginx 代理，然后 nginx 来负载所有 api server；以下为 nginx 代理相关配置 123456789101112131415161718192021222324252627282930313233# 创建配置目录mkdir -p /etc/nginx# 写入代理配置cat &lt;&lt; EOF &gt;&gt; /etc/nginx/nginx.conferror_log stderr notice;worker_processes auto;events &#123; multi_accept on; use epoll; worker_connections 1024;&#125;stream &#123; upstream kube_apiserver &#123; least_conn; server 192.168.7.11:6443; server 192.168.7.12:6443; server 192.168.7.13:6443; &#125; server &#123; listen 0.0.0.0:6443; proxy_pass kube_apiserver; proxy_timeout 10m; proxy_connect_timeout 1s; &#125;&#125;EOF# 更新权限chmod +r /etc/nginx/nginx.conf 为了保证 nginx 的可靠性，综合便捷性考虑，node 节点上的 nginx 使用 docker 启动，同时 使用 systemd 来守护， systemd 配置如下 12345678910111213141516171819202122232425cat &lt;&lt; EOF &gt;&gt; /etc/systemd/system/nginx-proxy.service[Unit]Description=kubernetes apiserver docker wrapperWants=docker.socketAfter=docker.service[Service]User=rootPermissionsStartOnly=trueExecStart=/usr/bin/docker run -p 127.0.0.1:6443:6443 \\ -v /etc/nginx:/etc/nginx \\ --name nginx-proxy \\ --net=host \\ --restart=on-failure:5 \\ --memory=512M \\ nginx:1.13.3-alpineExecStartPre=-/usr/bin/docker rm -f nginx-proxyExecStop=/usr/bin/docker stop nginx-proxyRestart=alwaysRestartSec=15sTimeoutStartSec=30s[Install]WantedBy=multi-user.targetEOF 启动 kubelet、nginx-proxy1234567systemctl daemon-reloadsystemctl start kubeletsystemctl enable kubeletsystemctl start kube-proxysystemctl enable kube-proxysystemctl start nginx-proxysystemctl enable nginx-proxy master 节点验证连通性1kubectl --server https://192.168.7.11:6443 --certificate-authority /etc/kubernetes/ssl/k8s-root-ca.pem --client-certificate /etc/kubernetes/ssl/admin.pem --client-key /etc/kubernetes/ssl/admin-key.pem get cs 12345678910111213# 查看 csr➜ kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-PhOGHB8BpFuNoYtnCGka4NTaxtDQDRjrctZtdaVsijY 2m kubelet-bootstrap Pending# 签发证书➜ kubectl certificate approve node-csr-PhOGHB8BpFuNoYtnCGka4NTaxtDQDRjrctZtdaVsijYcertificatesigningrequest &quot;node-csr-PhOGHB8BpFuNoYtnCGka4NTaxtDQDRjrctZtdaVsijY&quot; approved# 查看 node➜ kubectl get nodeNAME STATUS AGE VERSIONdocker4.node Ready 20s v1.7.4 安装 kubedns 插件安装 dashboard 插件安装 heapster 插件安装 EFK 插件部署 harbor 私有仓库]]></content>
      <categories>
        <category>devops</category>
        <category>kubernetes</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[悟透自己]]></title>
    <url>%2F2017%2F09%2F09%2F%E6%82%9F%E9%80%8F%E8%87%AA%E5%B7%B1%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;&nbsp;&nbsp;&nbsp;&nbsp;人生在世,和“自己”相处最多, 但是往往悟不透“自己”. &emsp;&emsp;&nbsp;&nbsp;&nbsp;&nbsp;人生走上坡路时,往往把自己估计过高,似乎一切所求的东西都能唾手可得,甚至把运气和机遇也看作自己身价的一部分而喜不自胜.人在不得意时,又往往把自己估计过低,把困难和不利也看做自己的无能,以至于把安分守己,与世无争误认为有自知之明,而实际上往往被怯懦的面具窒息了自己鲜活的生命. &emsp;&emsp;&nbsp;&nbsp;&nbsp;&nbsp;悟透自己,就是正确认识自己,也就是说要做一个冷静的现实主义者,既知道自己的优势,也知道自己的不足.我们可以憧憬人生,但期望值不能过高.因为在现实中,理想总是会打折扣的.可以迎接挑战.但是必须清楚自己努力的方向.也就是说,人一旦有了自知之明,也就没有什么克服不了的困难,没有什么过不去的难关. &emsp;&emsp;&nbsp;&nbsp;&nbsp;&nbsp;要悟透自己就要欣赏自己.无论你是一棵参天大树,还是一棵小草,无论你成为一座巍峨的高山,还是一快小小的石头,都是一种天然,都有自己存在的价值.只要你认真的欣赏自己,你就会拥有一个真正的自我.只有自我欣赏才会有信心,一旦拥有了信心也就会拥有了抵御一切逆境的动力. &emsp;&emsp;&nbsp;&nbsp;&nbsp;&nbsp;要悟透自己,就要心疼自己.在气愤时心疼一下自己,找个僻静处散散心,宣泄宣泄,不要让那些无名之火伤身;忧郁时,要心疼一下自己,找三五好友,诉说诉说,让感情的阴天变晴;劳累时,你要心疼一下自己,为自己来一番问寒问暖,要明白人所拥有的不过是一个血肉之躯,经不住太多的风力霜剑;有病时,你要心疼一下自己,惟有对自己的心疼,才是战胜疾病的信心和力量. &emsp;&emsp;&nbsp;&nbsp;&nbsp;&nbsp;悟透了自己,才能把握住自己,你生活才会有滋有味!]]></content>
      <categories>
        <category>ESSAYS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[JDK安装配置]]></title>
    <url>%2F2017%2F09%2F08%2FJDK%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[ubuntu1、解压文件并修改文件名 1sudo tar zxvf jdk-7u21-linux-i586.tar.gz -C /usr/lib/ 进入到安装目录中 1cd /usr/lib/ 修改文件名 1sudo mv jdk1.7.0_21 java 2、添加环境变量使用vim ~/.bashrc命令编辑 1234export JAVA_HOME=/usr/lib/javaexport JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/libexport PATH=$&#123;JAVA_HOME&#125;/bin:$PATH 3、激活环境变量 1source ~/.bashrc centos1、查看JDK信息 1rpm -qa | grep java 2、卸载OpenJDK——在 步骤 1中 复制所有的 openjdk卸载 1rpm -e --nodeps java-1.7.0-openjdk-headless-1.7.0.71-2.5.3.1.el7_0.x86_64 3、安装jdk 1sudo tar -zxvf yourjdk.tar.gz -C /usr/lib 4、配置JDK环境变量 1234export JAVA_HOME=/usr/lib/jdk1.8.0_92export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/binexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib 5、为了使编译生效 1source /etc/profile 6、查看path值 1echo $PATH Mac官网下载dmg版本安装后配置环境如下 1sudo vim ~/.bash_profile]]></content>
      <categories>
        <category>JAVA</category>
        <category>config</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F09%2F07%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
