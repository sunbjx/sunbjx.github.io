<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[理解消息通信之RabbitMQ]]></title>
    <url>%2F2017%2F09%2F26%2F%E7%90%86%E8%A7%A3%E6%B6%88%E6%81%AF%E9%80%9A%E4%BF%A1%E4%B9%8BRabbitMQ%2F</url>
    <content type="text"><![CDATA[消费者和生产者 RabbitMQ 在应用程序和服务器之间扮演着路由器等角色。所以当应用程序连接到RabbitMQ 时，它就必须做个决定：我是在发送还是接收呢？或者从AMQP的角度思考，我是一个生产者还是一个消费者呢？ 生产者(producer)创建消息，然后发布(发送)到代理服务器(RabbitMQ ). 什么是消息 消息包含两部分：有效荷载（payload）和标签（label） 有效荷载就是你想要传输的数据。它可以是任何内容。标签表述了有效荷载，并且RabbitMQ 用它来决定谁将获得消息的拷贝首先连接到Rabbit ，才能消费或者发布消息。你在应用程序和Rabbit 代理服务器之间创建一条TCP连接。一旦TCP连接打开（你通过了认证），应用程序就可以创建一条AMQP信道。 什么是信道 信道是建立在“真实的”TCP连接内的虚拟连接。AMQP命令都是通过信道发送出去的。每条信道都会指派一个唯一的ID（AMQP库会帮你记住ID的）不论是发布消息，订阅队列或是接收消息，这些动作都是通过信道完成的。 线程启动后，会在现成的连接上创建一条信道，也就获得了连接到Rabbit 上的私密通信路径，而不会给操作系统的TCP造成额外的负担。在一条TCP连接上创建多少条信道是没有限制的。 AMQP TCP连接就像电缆，而AMQP信道就像一条条独立光纤束。 AMQP消息路由必须有三部分：交换器、队列和绑定。生产者把消息发布到交换器上；消息最终达到队列，并被消费者接收；绑定决定了消息如何从路由器路由到特定的队列。 AMQP messaging 中的基本概念 Broker: 接收和分发消息的应用，RabbitMQ Server就是Message Broker。 Virtual host: 出于多租户和安全因素设计的，把AMQP的基本组件划分到一个虚拟的分组中，类似于网络中的namespace概念。当多个不同的用户使用同一个RabbitMQ server提供的服务时，可以划分出多个vhost，每个用户在自己的vhost创建exchange／queue等。 Connection: publisher／consumer和broker之间的TCP连接。断开连接的操作只会在client端进行，Broker不会断开连接，除非出现网络故障或broker服务出现问题。 Channel: 如果每一次访问RabbitMQ都建立一个Connection，在消息量大的时候建立TCP Connection的开销将是巨大的，效率也较低。Channel是在connection内部建立的逻辑连接，如果应用程序支持多线程，通常每个thread创建单独的channel进行通讯，AMQP method包含了channel id帮助客户端和message broker识别channel，所以channel之间是完全隔离的。Channel作为轻量级的Connection极大减少了操作系统建立TCP connection的开销。 Exchange: message到达broker的第一站，根据分发规则，匹配查询表中的routing key，分发消息到queue中去。常用的类型有：direct (point-to-point), topic (publish-subscribe) and fanout (multicast)。 Queue: 消息最终被送到这里等待consumer取走。一个message可以被同时拷贝到多个queue中。 Binding: exchange和queue之间的虚拟连接，binding中可以包含routing key。Binding信息被保存到exchange中的查询表中，用于message的分发依据。 典型的“生产／消费”消息模型 生产者发送消息到broker server（RabbitMQ）。在Broker内部，用户创建Exchange／Queue，通过Binding规则将两者联系在一起。Exchange分发消息，根据类型／binding的不同分发策略有区别。消息最后来到Queue中，等待消费者取走。 消息如何达到队列 当你想要将消息投递到队列时，你通过把消息发送给交换器来完成。然后根据确定的规则，RabbitMQ 将会决定消息该投递到哪个队列。这些规则被称为路由键（routing key）。 队列通过路由键绑定到交换器。当你把消息发送到代理服务器时，消息将拥有一个路由键（即便时空的）RabbitMQ 也会将其和绑定使用的路由键进行匹配。如果匹配，那么消息将会投递到该队列，如果路由的消息不匹配任何绑定模式的话，消息将进入“黑洞”。 服务器会根据路由键将消息从交换器路由到队列，但它是如何处理投递到多个队列的情况的呢？ 协议中定义的四种类型交换机：direct、fanout、topic、headers。 Exchange类型Exchange有多种类型，最常用的是Direct／Fanout／Topic三种类型。 Direct Message中的“routing key”如果和Binding中的“binding key”一致， Direct exchange则将message发到对应的queue中。 Fanout 每个发到Fanout类型Exchange的message都会分到所有绑定的queue上去。 Topic 根据routing key，及通配规则，Topic exchange将分发到目标queue中。 Routing key中可以包含两种通配符，类似于正则表达式： “#”通配任何零个或多个word “*”通配任何单个word 消费者通过一下两种方式从特定的队列中接收消息 通过AMQP的basic.consume命令订阅。这样做会将信道设置为接收模式，直到取消队列的订阅为止。订阅了消息后，消费者在消费（或者拒绝）最近接收的那条消息后，就能从队列中（可用的）自动接收下一条消息。 某些时候，你只想从队列获得单条消息而不是持续订阅。向队列请求单条消息是通过AMQP的basic.get命令实现的。如果要获得更多的消息，需要再次发送basic.get命令。 消费者理应始终使用basic.consume来实现高吞吐量。 如果至少有一个消费者订阅了队列的话，消息会立即发送给这些订阅的消费者。 如果消息到达了无人订阅的队列的话，消息会在队列中等待，一旦有消费者订阅到该队列，那么队列上的消息就会发送给消费者。 如果有多个消费者订阅到同一队列上时，消息如何分发： 当Rabbit队列拥有多个消费者时，队列收到的消息将以循环的方式发送给消费者。每条消息只会发送给一个订阅的消费者。 假设有seed_bin队列，消费者 A 和消费者 B订阅到seed_bin队列。当消息达到seed_bin队列时，消息投递方式如下： 1、消息message_a 达到 seed_bin 队列 2、RabbitMQ 把消息message_a 发送给A 3、A确认接收到了消息message_a 4、RabbitMQ 把消息message_a从seed_bin中删除 5、消息message_b达到seed_bin队列 6、RabbitMQ 把消息messages_b发送给B 7、B确认接收到了消息message_b 8、RabbitMQ 把消息message_b从seed_bin中删除 ACK 消息确认 消费者接收到的每一条消息都必须进行确认。消费者必须通过AMQP的basic.ack命令显式地向RabbitMQ 发送一个确认，或者在订阅到队列的时候将basic.ack参数设置为true。 消费者通过确认命令告诉RabbitMQ 它已经正确地接收了消息，同时RabbitMQ 才能安全地把消息从队列中删除。 如果消费者收到一条消息，然后确认之前从Rabbit断开连接（或者从队列上取消订阅），RabbitMQ 会认为这条消息没有分发，然后重新分发给下一个订阅的消费者。 如果应用程序有bug而忘记确认消息的话，Rabbit将不会给该消费者发送更多消息了。 只要消息尚未确认，则有以下两个选择： 把消费者从RabbitMQ 服务器断开连接。 使用AMQP的basic.reject命令 A、如果把reject命令的requeue参数设置成true，RabbitMQ 会将消息重新发送给下一个消 费者， B、如果设置成false，RabbitMQ 立即会把消息从队列中移除，而不会把它发送给新的消费者。 队列创建队列：消费者和生产者都能使用AMQP的queue.declare命令来创建队列。但是如果消费者在同一条信道上订阅了另一个队列的话，就无法再声明队列了。必须先取消订阅，将信道设置为“传输”模式。 实用参数： exclusive 如果设置为true，队列将变成私有的，此时只有你的应用程序才能够消费队列消息。 auto-delete 当最后一个消费者取消订阅时，队列会自动移除。 检测队列是否存在：将queue.declare的passive选项为true 如果队列存在，那么queue.declare命令会成功返回 如果队列不存在，queue.declare命令不会创建队列而会返回一个错误。 队列是AMQP消息通信的基础模块： 为消息提供了处所，消息在此等待消费 对负载均衡来说，队列是绝佳方案，只需附加一堆消费者，并让RabbitMQ 以循环的方式均匀的分配发来的消息。 队列是Rabbit中消息的终点（除非消息进入了“黑洞”） 服务器必须实现direct类型交换器，包含一个空白字符串名称的默认交换器。当声明一个队列时，它会自动绑定到默认交换器，并以队列名称做为路由键。 多租户模式：虚拟主机和隔离 每一个RabbitMQ 服务器都能创建虚拟消息服务器，我们称之为虚拟主机（vhost）. vhost之于Rabbit就像虚拟机之于物理服务器一样。 当在RabbitMQ 集群上创建vhost时，整个集群上都会创建该vhost。 消息持久化重启RabbitMQ 服务器后，那些队列和交换器就都消失了。原因在于每个队列和交换器的durable属性。 该属性默认情况为false，它决定了RabbitMQ 是否需要在崩溃或者重启之后重新创建队列或者交换器。 设置为true，就不需要在服务器断电后重新创建队列和交换器了。 能从AMQP服务器崩溃中恢复的消息，我们称之为持久化消息。 在消息发布前，通过把它的“投递模式”（delivery mode）选项设置为 2来把消息标记成持久化。 到目前为止，消息还只是被表示为持久化的，但是它还必须被发布到持久化的交换器中并到达持久化的队列中才行。 消息要从Rabbit崩溃中恢复 把它的投递模式选项设置为2（持久） 发送到持久化的交换器 到达持久化队列 RabbitMQ 确保持久性消息能从服务器重启中恢复的方式是，将它们写入磁盘上的一个持久化日志文件。当发布一条持久性消息到持久交换器上时，Rabbit消息会在消息提交到日志文件后才发送响应。 注意： 之后这条消息如果路由到了非持久队列的话，它会自动从持久性日志中移除，并且无法从服务器重启中恢复。 一旦从持久化队列中消费了一条持久性消息到话（并且确认了它），RabbitMQ 会在持久化日志中把这条消息标记为等待垃圾收集。 权衡取舍，什么情况下应该使用持久化消息通信： 分析并测试性能需求 是否需要单台Rabbit服务器每秒处理10 0000 条消息 发送方确认模式需要告诉Rabbit将信道设置成confirm模式，而且只能通过重新创建信道来关闭该设置。 一旦信道进入confirm模式，所有在信道上发布的消息都会被指派一个唯一的ID号（从 1 开始）。 一旦消息被投递给所有匹配的队列后，信道会发送一个发送方确认模式给生产者应用程序（包含消息的唯一ID）这使得生产者知晓消息已经安全到达目的队列了。 如果消息和队列是可持久化的，那么确认消息只会在队列将消息写入磁盘后才会发出。 发送方确认模式最大的好处是它们是异步 的。 于 《RabbitMQ实战》抄录]]></content>
      <categories>
        <category>middleware</category>
        <category>RabbitMQ</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[docker私有仓库之Nexus3]]></title>
    <url>%2F2017%2F09%2F25%2Fdocker%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93%E4%B9%8BNexus3%2F</url>
    <content type="text"><![CDATA[获取Nexus的Docker镜像Docker hub -nexus 1$ docker pull sonatype/nexus3 容器运行Nexus12$ mkdir -p /data/nexus-data &amp;&amp; chown -R 200 /data/nexus-data$ docker run -d -p 8081-8100:8081-8100 --name nexus -v /data/nexus-data:/nexus-data sonatype/nexus3 创建Docker私有仓库 默认账户 admin 密码 admin123，登录如下 在设置 Repositories 选项卡中中选择 Create repository 选择hosted hosted: 本地存储，即同 docker 官方仓库一样提供本地私服功能 proxy: 提供代理其他仓库的类型，如 docker 中央仓库 group: 组类型，实质作用是组合多个仓库为一个地址 选择 hosted 类型仓库，然后输入一个仓库名，并勾选 HTTP 选项，端口任意即可 测试私服 测试 push 和 pull 镜像 12345678910111213141516171819202122docker tag sunbjx/alpine registry.com:8082/alpine➜ ~ docker push registry.com:8082/alpineThe push refers to a repository [registry.com:8082/alpine]754684812d65: Pushed60ab55d3379d: Pushedlatest: digest: sha256:28f397aca53eb3e8ea1627f4af9c262fca7db17f0c6db492b53adc7bca7d0f91 size: 739➜ ~ docker rmi registry.com:8082/alpineUntagged: registry.com:8082/alpine:latestUntagged: registry.com:8082/alpine@sha256:28f397aca53eb3e8ea1627f4af9c262fca7db17f0c6db492b53adc7bca7d0f91➜ ~ docker rmi sunbjx/alpineUntagged: sunbjx/alpine:latestUntagged: sunbjx/alpine@sha256:28f397aca53eb3e8ea1627f4af9c262fca7db17f0c6db492b53adc7bca7d0f91Deleted: sha256:090c790ee6f28f495d92d5be43641573b0d1b5502b35f7662d88cdbf8d548afdDeleted: sha256:378e2b887fcdffcbd113a7cf6f97e9f8a58851b0a205b31a93acdb887912850d➜ ~ docker pull registry.com:8082/alpineUsing default tag: latestlatest: Pulling from alpine0a8490d0dfd3: Already exists8fb018fb4173: Pull completeDigest: sha256:28f397aca53eb3e8ea1627f4af9c262fca7db17f0c6db492b53adc7bca7d0f91Status: Downloaded newer image for registry.com:8082/alpine:latest]]></content>
      <categories>
        <category>devops</category>
        <category>docker</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CentOS7源码安装mysql5.6(社区版)]]></title>
    <url>%2F2017%2F09%2F25%2FCentOS7%E6%BA%90%E7%A0%81%E5%AE%89%E8%A3%85mysql5-6-%E7%A4%BE%E5%8C%BA%E7%89%88%2F</url>
    <content type="text"><![CDATA[确认安装版本mysql分为开发版本和稳定版本（GA），开发版本拥有最新的特性，但是并不稳定，也没有完全经过测试，可能存在严重的bug，而稳定版本是经过了长时间的测试，消除了具有已知的bug，其稳定性和安全性都得到一定的保障。 对于一个mysql的版本号如：mysql-5.6.1-m1，这个版本号意味着什么呢？ 对于5.6.1的解释：第一个数字5代表了文件格式，第二个数字6代表了发行级别，第三个数字1代表了版本号。更新幅度较小时，最后的数字会增加，出现了重大特性更新时，第二个数字会增加，文件格式改变时，第一个数字会增加 对于m1的解释：这是用来表明这个mysql版本的稳定性级别的，如果没有这个后缀，那么这个版本就是一个稳定版（GA）；如果这个后缀是mN（例如m1，m2）格式，表明了这个版本加入了一些经过彻底测试的新特性，可以认为这是一个试生产的模具；如果这个后缀是rc，表明了这是一个候选版本，已经修改了已知的重要bug，但是没有经过足够长时间的使用来确认所有的bug已经被修复。 一旦选择了版本号，就要选择使用哪个发行版，你可以使用二进制发行版如RPM包或Zip压缩包等，但是如果你要实现如下的功能，就要选择源码安装（本文正是选择源码安装的方式）： 把mysq安装到指定位置 使用mysql的一些特性（标准的二进制版本中并没有这些特性）如：TCP封包支持，调试mysql 二进制版本中默认支持所有的字符集，但你可以在编译安装源码时指定字符集，从而使得安装的mysql更小 卸载原有MySQL或者MariadbCentOs7版本默认情况下安装了mariadb-libs，必须先卸载才可以继续安装MySql 查找以前是否安装mariadb-libs如：mariadb-libs-5.5.35-3.el7.x86_64 1$ rpm -qa | grep -i mariadb-libs 卸载已经安装的mariadb-libs1$ yum remove mariadb-libs-5.5.35-3.el7.x86_64 查找以前是否安装MySQL1$ rpm -qa | grep -i mysql 如果显示有数据 说明已经安装了 MySQL 程序 停止mysql服务 1$ service mysql stop 删除之前安装的mysql12$ rpm -ve 文件名称 例如：MySQL-server-5.6.24-1.linux_glibc2.5.x86_64$ rpm -ve 文件名称 例如：MySQL-client-5.6.24-1.linux_glibc2.5.x86_64 查找之前老版本mysql的目录、并且删除老版本mysql的文件和库1$ find / -name mysql 如： /var/lib/mysql /usr/lib64/mysql /usr/local/mysql /usr/local/mysql/data/mysql 删除对应的目录 123$ rm -rf /var/lib/mysql $ rm -rf /usr/lib64/mysql $ rm -rf /usr/local/mysql 删除配置文档 1$ rm -rf /etc/my.cnf 下载MySQL下载完后需要检查文件的MD5，以确认是否从官网下载的原版本（以防被人篡改过该软件） 我选择如下：官网地址 编译和安装MySQL安装环境12345$ yum -y install make bison-devel ncures-devel libaio$ yum -y install libaio libaio-devel$ yum -y install perl-Data-Dumper$ yum -y install net-tools$ yum -i install bison gcc-c++ cmake ncurses 解压 1$ tar -zxvf mysql-5.6.32.tar.gz 编译安装123$ cmake \-DCMAKE_INSTALL_PREFIX=/usr/local/mysql56 -DMYSQL_DATADIR=/usr/local/mysql/data -DSYSCONFDIR=/etc/my.cnf -DWITH_MYISAM_STORAGE_ENGINE=1 -DWITH_INNOBASE_STORAGE_ENGINE=1 -DWITH_MEMORY_STORAGE_ENGINE=1 -DWITH_READLINE=1 -DMYSQL_UNIX_ADDR=/tmp/mysqld.sock -DMYSQL_TCP_PORT=3306 -DENABLED_LOCAL_INFILE=1 -DWITH_PARTITION_STORAGE_ENGINE=1 -DEXTRA_CHARSETS=all -DDEFAULT_CHARSET=utf8 -DDEFAULT_COLLATION=utf8_general_ci$ make &amp;&amp; make install 123456789101112131415161718192021222324252627# -DCMAKE_INSTALL_PREFIX=/usr/local/mysql56 \ #安装路径 # -DMYSQL_DATADIR=/usr/local/mysql/data \ #数据文件存放位置 # -DSYSCONFDIR=/etc \ #my.cnf路径 # -DWITH_MYISAM_STORAGE_ENGINE=1 \ #支持MyIASM引擎 # -DWITH_INNOBASE_STORAGE_ENGINE=1 \ #支持InnoDB引擎 # -DWITH_MEMORY_STORAGE_ENGINE=1 \ #支持Memory引擎 # -DWITH_READLINE=1 \ #快捷键功能(我没用过) # -DMYSQL_UNIX_ADDR=/tmp/mysqld.sock \ #连接数据库socket路径 # -DMYSQL_TCP_PORT=3306 \ #端口 # -DENABLED_LOCAL_INFILE=1 \ #允许从本地导入数据 # -DWITH_PARTITION_STORAGE_ENGINE=1 \ #安装支持数据库分区 # -DEXTRA_CHARSETS=all \ #安装所有的字符集 # -DDEFAULT_CHARSET=utf8 \ #默认字符 # -DDEFAULT_COLLATION=utf8_general_ci 配置MySQL 检查系统是否已经有mysql用户，如果没有则创建 12$ cat /etc/passwd | grep mysql$ cat /etc/group | grep mysql 创建mysql用户（但是不能使用mysql账号登陆系统 12$ groupadd mysql$ useradd -g -s mysql mysql 修改权限 123456789$ chown -R mysql:mysql /usr/local/mysql$ cd /usr/local/mysql$ chown -R mysql:mysql .$ scripts/mysql_install_db --user=mysql# 将权限设置给root用户，并设置给mysql组， 取消其他用户的读写执行权限，# 仅留给mysql &quot;rx&quot;读执行权限，其他用户无任何权限$ chown -R root:mysql .$ chown -R mysql:mysql ./data$ chmod -R ug+rwx . 将mysql的配置文件拷贝到/etc 123$ cp support-files/my-default.cnf /etc/my.cnf# 注意：5.6 之前如下$ cp support-files/my-medium.cnf /etc/my.cnf 修改my.cnf配置 1$ vim /etc/my.cnf [mysqld] 下面添加： user=mysql datadir=/data/mysql default-storage-engine=MyISAM 启动mysql 123# 将mysql的启动服务添加到系统服务中$ cp support-files/mysql.server /etc/init.d/mysql$ service mysql start 增加MySQL服务 1$ chkconfig --add mysql 修改root用户密码 123$ cd /usr/local/mysql56$ ./bin/mysqladmin -u root password$ service mysql restart 常用基础操作12345678910111213$ mysql -u root -p root-- 查看数据库show databases;-- 创建用户并允许本地用户通过密码登录create user &apos;username&apos;@&apos;localhost&apos; identified by &apos;user-password&apos;;-- 创建数据库create database schema_name;-- 不可授权给其他用户(* 表示该数据库下所有的表名)grant all privileges on schema_name.* to &apos;username&apos;@&apos;remote-ip&apos; identified by &apos;remote-user-password&apos; -- 可授权给其他用户grant all privileges on schema_name.* to &apos;username&apos;@&apos;remote-ip&apos; identified by &apos;remote-user-password&apos; with grant option;-- 刷新flush privileges;]]></content>
      <categories>
        <category>DB</category>
        <category>MySQL</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Centos7下RabbitMQ服务安装配置]]></title>
    <url>%2F2017%2F09%2F22%2FCentos7%E4%B8%8BRabbitMQ%E6%9C%8D%E5%8A%A1%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[安装 Erlang下载源码(官网地址:)1$ wget http://erlang.org/download/otp_src_19.3.tar.gz 安装依赖1$ yum install gcc glibc-devel make ncurses-devel openssl-devel xmlto 解压第一步下载的源码12$ tar zxvf otp_src_19.3.tar.gz$ cd opt_src_19.3 配置安装路径编译代码1$ ./configure --prefix=/opt/erlang 执行编译结果1$ make &amp;&amp; make install 验证1$ ./opt/erlang/bin/erl 设置环境变量vim /etc/profile12#set erlang environmentexport PATH=$PATH:/opt/erlang/bin 编译环境变量1$ source /etc/profile 安装RabbitMQ下载(官网地址:)1$ wget http://www.rabbitmq.com/releases/rabbitmq-server/v3.6.11/rabbitmq-server-generic-unix-3.6.11.tar.xz 解压文件并重命名1234$ xz -d rabbitmq-server-generic-unix-3.6.1.tar.xz$ tar -xvf rabbitmq-server-generic-unix-3.6.1.tar -C /opt$ cd /opt$ mv rabbitmq_server-3.6.11 rabbitmq 设置环境变量vim /etc/profile12#set rabbitmq environmentexport PATH=$PATH:/opt/rabbitmq/sbin 编译环境变量1$ source /etc/profile 创建日志文件RabbitMQ使用Mnesia数据库存储服务器信息，比如队列元数据，虚拟主机等. 12$ mkdir -p /var/log/rabbitmq$ mkdir -p /var/log/rabbitmq/mnesia/rabbit 启动服务12# cd /opt/rabbitmq/sbin$ ./rabbitmq-server -detached 配置网页插件1$ ./rabbitmq-plugins enable rabbitmq_management 远程访问配置 添加用户:rabbitmqctl add_user hxb hxb 添加权限:rabbitmqctl set_permissions -p &quot;/&quot; hxb &quot;.*&quot; &quot;.*&quot; &quot;.*&quot; 修改用户角色:rabbitmqctl set_user_tags hxb administrator RabbitMQ的用户角色分类RabbitMQ的用户角色分类：none、management、policymaker、monitoring、administrator 角色-none 不能访问 management plugin 角色-management 用户可以通过AMQP做的任何事外加： 列出自己可以通过AMQP登入的virtual hosts 查看自己的virtual hosts中的queues, exchanges 和 bindings 查看和关闭自己的channels 和 connections 查看有关自己的virtual hosts的“全局”的统计信息，包含其他用户在这些virtual hosts中的活动。 角色-policymaker management可以做的任何事外加： 查看、创建和删除自己的virtual hosts所属的policies和parameters 角色-monitoring management可以做的任何事外加： 列出所有virtual hosts，包括他们不能登录的virtual hosts 查看其他用户的connections和channels 查看节点级别的数据如clustering和memory使用情况 查看真正的关于所有virtual hosts的全局的统计信息 角色-administrator policymaker和monitoring可以做的任何事外加: 创建和删除virtual hosts 查看、创建和删除users 查看创建和删除permissions 关闭其他用户的connections RabbitMQ常用命令add_user &lt;UserName&gt; &lt;Password&gt; delete_user &lt;UserName&gt; change_password &lt;UserName&gt; &lt;NewPassword&gt; list_users add_vhost &lt;VHostPath&gt; delete_vhost &lt;VHostPath&gt; list_vhostsset_permissions [-p &lt;VHostPath&gt;] &lt;UserName&gt; &lt;Regexp&gt; &lt;Regexp&gt; &lt;Regexp&gt; clear_permissions [-p &lt;VHostPath&gt;] &lt;UserName&gt; list_permissions [-p &lt;VHostPath&gt;] list_user_permissions &lt;UserName&gt; list_queues [-p &lt;VHostPath&gt;] [&lt;QueueInfoItem&gt; ...] list_exchanges [-p &lt;VHostPath&gt;] [&lt;ExchangeInfoItem&gt; ...] list_bindings [-p &lt;VHostPath&gt;] list_connections [&lt;ConnectionInfoItem&gt; ...]]]></content>
      <categories>
        <category>middleware</category>
        <category>RabbitMQ</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[我让自己自由了三年]]></title>
    <url>%2F2017%2F09%2F18%2F%E6%88%91%E8%AE%A9%E8%87%AA%E5%B7%B1%E8%87%AA%E7%94%B1%E4%BA%86%E4%B8%89%E5%B9%B4%2F</url>
    <content type="text"><![CDATA[我让自己自由了三年-迟来的人生规划（未来的3年计划5年规划） 感觉这一刻才刚开始（毕业后的面试中总会遇到你未来的五年规划或者计划是什么？支支吾吾了下-😅 接着问到你未来三年是什么或者接下来一年是什么？接着😅） 就这样尴尬了三年（工作上一直在被动工作，学习上几乎没有，最后经历了由恋爱到寂寞与孤独） 2018-2020年 我的人生第一个三年计划2021-2025年 我的人生第一个五年规划2018-2020年 我要在成都在英孚把我的英语培训起来（至少要养成一个每天学英语的好习惯）；工作上我必须在devops这一块不断的夯实自己 加倍时间去完成（主打java 涉及python go js）；生活里能否找到自己心仪的适合结婚的好女孩😊；在父母的帮助下必须一定得自己去在成都按揭一套心仪的房子；车能不买就不买 买了也是吃灰😂🤦‍♂️2021-2025年 在一个自己喜欢的公司把它当成自己的公司尽我所能看看我能为公司创造多大的效益并且对比一下公司为我的花费（公司地址不局限于国内😄💪） 这时候自己35岁了必须得有一个幸福的家庭（在此之前能够兼顾到并完成当然更好😂💪）；选择与能够非常非常非常信任的朋友大家一条心的去创业（产品方向能够为社会为人们带来便利与好处）or 能遇到一个理想的好平台达到和自己创业一致的能够高速高质量稳定的驶向远方 让自己对这个世界甚至这茫茫星云中了解的更多探索的更多 这一切都必须得身体健康⛽️ 大叔阶段能把妹，花甲之年能抱孙子孙女，有生之年能见重孙重孙女 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;—程序员也有的幸福]]></content>
      <categories>
        <category>规划与计划</category>
        <category>人生规划</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[在CentOS7上部署kubernetes1.7 HA 集群(Vagrant)]]></title>
    <url>%2F2017%2F09%2F13%2F%E5%9C%A8CentOS7%E4%B8%8A%E9%83%A8%E7%BD%B2kubernetes1-7-HA-%E9%9B%86%E7%BE%A4-Vagrant%2F</url>
    <content type="text"><![CDATA[使用virtualbox + vagrant安装centos7vagrant准本工作1、具体安装和使用virtualbox &amp;&amp; vagrant请自行Google 2、搜索centos7 box:https://app.vagrantup.com/boxes/search?provider=virtualbox 3、生成Vagrantfile 123mkdir -p ~/Vagrant/k8s-clustercd ~/Vagrant/k8s-clustervagrant init centos/7 安装centos71、编辑Vagrantfile 123cd ~/Vagrant/k8s-clustercp Vagrantfile ./Vagrantfile_backupemacs Vagrantfile Vagrantfile 内容如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081# coding: utf-8Vagrant.configure(&quot;2&quot;) do |config| (11..13).each do |i| config.vm.define &quot;k8s-master#&#123;i&#125;&quot; do |node| # 设置虚拟机的Box node.vm.box = &quot;centos/7&quot; # 设置虚拟机的主机名 node.vm.hostname=&quot;master#&#123;i&#125;&quot; # 设置虚拟机的IP node.vm.network &quot;public_network&quot;, ip: &quot;192.168.7.#&#123;i&#125;&quot;, :bridge=&gt;&apos;en0: Wi-Fi (AirPort)&apos; node.vm.boot_timeout = 20 # 设置主机与虚拟机的共享目录 node.vm.synced_folder &quot;/Users/sunbjx/Vagrant/share&quot;, &quot;/home/vagrant/share&quot; # VirtaulBox相关配置 node.vm.provider &quot;virtualbox&quot; do |v| # 设置虚拟机的名称 v.name = &quot;k8s-master#&#123;i&#125;&quot; # 设置虚拟机的内存大小 v.memory = 4096 # 设置虚拟机的CPU个数 v.cpus = 2 end # 使用shell脚本进行软件安装和配置 node.vm.provision &quot;shell&quot;, inline: &lt;&lt;-SHELL sudo yum -y update sudo yum -y install net-tools sudo yum -y install vim SHELL end end (14..15).each do |i| config.vm.define &quot;k8s-node#&#123;i&#125;&quot; do |node| # 设置虚拟机的Box node.vm.box = &quot;centos/7&quot; # 设置虚拟机的主机名 node.vm.hostname=&quot;node#&#123;i&#125;&quot; # 设置虚拟机的IP node.vm.network &quot;public_network&quot;, ip: &quot;192.168.7.#&#123;i&#125;&quot;, :bridge=&gt;&apos;en0: Wi-Fi (AirPort)&apos; node.vm.boot_timeout = 20 # 设置主机与虚拟机的共享目录 node.vm.synced_folder &quot;/Users/sunbjx/Vagrant/share&quot;, &quot;/home/vagrant/share&quot; # VirtaulBox相关配置 node.vm.provider &quot;virtualbox&quot; do |v| # 设置虚拟机的名称 v.name = &quot;k8s-node#&#123;i&#125;&quot; # 设置虚拟机的内存大小 v.memory = 8192 # 设置虚拟机的CPU个数 v.cpus = 2 end # 使用shell脚本进行软件安装和配置 node.vm.provision &quot;shell&quot;, inline: &lt;&lt;-SHELL sudo yum -y update sudo yum -y install net-tools sudo yum -y install vim SHELL end endend 2、启动安装centos7 1vagrant up 环境说明环境总共 5 台虚拟机，3 个 master，3 个 etcd 节点，2 个 node 网络方案这里采用flannel，集群开启 RBAC IP 节点 192.168.7.11 master、etcd 192.168.7.12 master、etcd 192.168.7.13 master、etcd 192.168.7.14 node 192.168.7.15 node 创建 TLS证书和秘钥证书说明 证书名称 配置文件 用途 etcd-root-ca.pem etcd-root-ca-csr.json etcd 根 CA 证书 etcd.pem etcd-gencert.json、etcd-csr.json etcd 集群证书 k8s-root-ca.pem k8s-root-ca-csr.json k8s 根 CA 证书 kube-proxy.pem k8s-gencert.json、kube-proxy-csr.json kube-proxy 使用的证书 admin.pem k8s-gencert.json、admin-csr.json kubectl 使用的证书 kubernetes.pem k8s-gencert.json、kubernetes-csr.json kube-apiserver 使用的证书 安装CFSSL (192.168.7.11)12345678910111213mkdir -p ~/downloadcd ~/download# 下载 go1.8.3wget https://golang.org/dl/go1.8.3.linux-amd64.tar.gz# 解压压缩包tar -C /usr/local -xzf go1.8.3.linux-amd64.tar.gz# 添加环境变量vim /etc/profileexport GOROOT=/usr/local/goexport PATH=$PATH:$GOROOT/binsource /etc/profile# 使用 go 命令安装 cfsslgo get -u github.com/cloudflare/cfssl/cmd/... 生成etcd证书 (192.168.7.11) etcd 证书配置文件 12mkdir -p ~/document/sslcd ~/document/ssl vim etcd-root-ca-csr.json 12345678910111213141516&#123; &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 4096 &#125;, &quot;names&quot;: [ &#123; &quot;O&quot;: &quot;etcd&quot;, &quot;OU&quot;: &quot;etcd Security&quot;, &quot;L&quot;: &quot;Beijing&quot;, &quot;ST&quot;: &quot;Beijing&quot;, &quot;C&quot;: &quot;CN&quot; &#125; ], &quot;CN&quot;: &quot;etcd-root-ca&quot;&#125; vim etcd-gencert.json 12345678910111213&#123; &quot;signing&quot;: &#123; &quot;default&quot;: &#123; &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot; ], &quot;expiry&quot;: &quot;87600h&quot; &#125; &#125;&#125; vim etcd-csr.json 12345678910111213141516171819202122232425&#123; &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 4096 &#125;, &quot;names&quot;: [ &#123; &quot;O&quot;: &quot;etcd&quot;, &quot;OU&quot;: &quot;etcd Security&quot;, &quot;L&quot;: &quot;Beijing&quot;, &quot;ST&quot;: &quot;Beijing&quot;, &quot;C&quot;: &quot;CN&quot; &#125; ], &quot;CN&quot;: &quot;etcd&quot;, &quot;hosts&quot;: [ &quot;127.0.0.1&quot;, &quot;localhost&quot;, &quot;192.168.1.11&quot;, &quot;192.168.1.12&quot;, &quot;192.168.1.13&quot;, &quot;192.168.1.14&quot;, &quot;192.168.1.15&quot; ]&#125; “CN”：Common Name，kube-apiserver 从证书中提取该字段作为请求的用户名 (User Name)；浏览器使用该字段验证网站是否合法；“O”：Organization，kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group) 生成 etcd 证书 12cfssl gencert --initca=true etcd-root-ca-csr.json | cfssljson --bare etcd-root-cacfssl gencert --ca etcd-root-ca.pem --ca-key etcd-root-ca-key.pem --config etcd-gencert.json etcd-csr.json | cfssljson --bare etcd 生成 kubernetes 证书 (192.168.7.11) kubernetes 证书配置文件 vim k8s-root-ca-csr.json 12345678910111213141516&#123; &quot;CN&quot;: &quot;kubernetes&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 4096 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;BeiJing&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125; vim k8s-gencert.json 123456789101112131415161718&#123; &quot;signing&quot;: &#123; &quot;default&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot; &#125;, &quot;profiles&quot;: &#123; &quot;kubernetes&quot;: &#123; &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot; ], &quot;expiry&quot;: &quot;87600h&quot; &#125; &#125; &#125;&#125; vim kubernetes-csr.json 12345678910111213141516171819202122232425262728293031&#123; &quot;CN&quot;: &quot;kubernetes&quot;, &quot;hosts&quot;: [ &quot;127.0.0.1&quot;, &quot;10.254.0.1&quot;, &quot;192.168.7.11&quot;, &quot;192.168.7.12&quot;, &quot;192.168.7.13&quot;, &quot;192.168.7.14&quot;, &quot;192.168.7.15&quot;, &quot;localhost&quot;, &quot;kubernetes&quot;, &quot;kubernetes.default&quot;, &quot;kubernetes.default.svc&quot;, &quot;kubernetes.default.svc.cluster&quot;, &quot;kubernetes.default.svc.cluster.local&quot; ], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;BeiJing&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125; vim kube-proxy-csr.json 1234567891011121314151617&#123; &quot;CN&quot;: &quot;system:kube-proxy&quot;, &quot;hosts&quot;: [], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;BeiJing&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125; vim admin-csr.json 1234567891011121314151617&#123; &quot;CN&quot;: &quot;admin&quot;, &quot;hosts&quot;: [], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;BeiJing&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;O&quot;: &quot;system:masters&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125; 生成 kubernetes 证书 12345cfssl gencert --initca=true k8s-root-ca-csr.json | cfssljson --bare k8s-root-cafor targetName in kubernetes admin kube-proxy; do cfssl gencert --ca k8s-root-ca.pem --ca-key k8s-root-ca-key.pem --config k8s-gencert.json --profile kubernetes $targetName-csr.json | cfssljson --bare $targetNamedone 分发所需证书到各个节点目录 /etc/kubernetes/ssl 下面 123456789for IP in `seq 1 5`;do ssh root@192.168.1.1$IP mkdir -p /etc/etcd/ssl sudo scp ~/document/ssl/etcd*.pem root@192.168.1.1$IP:/etc/etcd/ssl ssh root@192.168.1.1$IP chown -R etcd:etcd /etc/etcd/ssl ssh root@192.168.1.1$IP chmod -R 755 /etc/etcd ssh root@192.168.7.1$IP mkdir -p /etc/kubernetes/ssl sudo scp ~/document/ssl/*.pem root@192.168.7.1$IP:/etc/kubernetes/ssl ssh root@192.168.7.1$IP chown -R kube:kube /etc/kubernetes/ssldone 生成 token 及 kubeconfig (192.168.7.11)kubelet、kube-proxy 等 Node 机器上的进程与 Master 机器的 kube-apiserver 进程通信时需要认证和授权；kubernetes 1.4 开始支持由 kube-apiserver 为客户端生成 TLS 证书的 TLS Bootstrapping 功能，这样就不需要为每个客户端生成证书了；该功能当前仅支持为 kubelet 生成证书 生成 token 12345cd /etc/kubernetesexport BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d &apos; &apos;)cat &gt; token.csv &lt;&lt;EOF$&#123;BOOTSTRAP_TOKEN&#125;,kubelet-bootstrap,10001,&quot;system:kubelet-bootstrap&quot;EOF BOOTSTRAP_TOKEN 将被写入到 kube-apiserver 使用的 token.csv 文件和 kubelet 使用的 bootstrap.kubeconfig 文件，如果后续重新生成了 BOOTSTRAP_TOKEN，则需要：更新 token.csv 文件，分发到所有机器 (master 和 node）的 /etc/kubernetes/ 目录下，分发到node节点上非必需；重新生成 bootstrap.kubeconfig 文件，分发到所有 node 机器的 /etc/kubernetes/ 目录下；重启 kube-apiserver 和 kubelet 进程；重新 approve kubelet 的 csr 请求 安装 kubernetes 并将 kube-apiserver,kube-controller-manager,kube-scheduler,kubectl 分发到 master; kubelet,kube-proxy 分发到 node; 同时分发所需证书到各个节点目录 /etc/kubernetes/ssl 下面 123456789101112131415161718cd ~/downloadwget https://github.com/kubernetes/kubernetes/releases/download/v1.7.4/kubernetes.tar.gztar -zxvf kubernetes.tar.gzcd kubernetes./cluster/get-kube-binaries.shcd kubernetes/servertar -zxvf kubernetes-server-linux-amd64.tar.gzcd kubernetes/server/binfor IP in `seq 1 3`;do sudo scp ./&#123;kube-apiserver,kube-controller-manager,kube-scheduler,kubectl&#125; root@192.168.7.1$IP:/usr/local/bin/ ssh root@192.168.7.1$IP chmod a+x /usr/local/bin/kube*donefor IP in `seq 4 5`;do sudo scp ./&#123;kubelet,kube-proxy&#125; root@192.168.7.1$IP:/usr/local/bin ssh root@192.168.7.1$IP chmod a+x /usr/local/bin/kube*done 创建 kubelet bootstrapping kubeconfig 文件 12345678910111213141516171819cd /etc/kubernetesexport KUBE_APISERVER=&quot;https://192.168.7.11:6443&quot;# 设置集群参数kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/ssl/k8s-root-ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=bootstrap.kubeconfig# 设置客户端认证参数kubectl config set-credentials kubelet-bootstrap \ --token=$&#123;BOOTSTRAP_TOKEN&#125; \ --kubeconfig=bootstrap.kubeconfig# 设置上下文参数kubectl config set-context default \ --cluster=kubernetes \ --user=kubelet-bootstrap \ --kubeconfig=bootstrap.kubeconfig# 设置默认上下文kubectl config use-context default --kubeconfig=bootstrap.kubeconfig –embed-certs 为 true 时表示将 certificate-authority 证书写入到生成的 bootstrap.kubeconfig 文件中；设置客户端认证参数时没有指定秘钥和证书，后续由 kube-apiserver 自动生成 创建 kube-proxy kubeconfig 文件 12345678910111213141516171819# 设置集群参数kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/ssl/k8s-root-ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=kube-proxy.kubeconfig# 设置客户端认证参数kubectl config set-credentials kube-proxy \ --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \ --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \ --embed-certs=true \ --kubeconfig=kube-proxy.kubeconfig# 设置上下文参数kubectl config set-context default \ --cluster=kubernetes \ --user=kube-proxy \ --kubeconfig=kube-proxy.kubeconfig# 设置默认上下文kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig 设置集群参数和客户端认证参数时 –embed-certs 都为 true，这会将 certificate-authority、client-certificate 和 client-key 指向的证书文件内容写入到生成的 kube-proxy.kubeconfig 文件中；kube-proxy.pem 证书中 CN 为 system:kube-proxy，kube-apiserver 预定义的 RoleBinding cluster-admin 将User system:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限 分发 token、kubeconfig 文件 1234for IP in `seq 2 5`;do scp *.kubeconfig root@192.168.7.1$IP:/etc/kubernetes scp token.csv root@192.168.7.1$IP:/etc/kubernetesdone 部署 HA etcd安装 etcd123sudo yum -y install etcdecho &apos;export ETCDCTL_API=3&apos; &gt;&gt; /etc/profilesource /etc/profile 修改配置 vim /etc/etcd/etc.conf123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# [member]ETCD_NAME=etcd1ETCD_DATA_DIR=&quot;/var/lib/etcd/etcd1.etcd&quot;ETCD_WAL_DIR=&quot;/var/lib/etcd/wal&quot;ETCD_SNAPSHOT_COUNT=&quot;100&quot;ETCD_HEARTBEAT_INTERVAL=&quot;100&quot;ETCD_ELECTION_TIMEOUT=&quot;1000&quot;ETCD_LISTEN_PEER_URLS=&quot;https://192.168.7.11:2380&quot;ETCD_LISTEN_CLIENT_URLS=&quot;https://192.168.7.11:2379,http://127.0.0.1:2379&quot;ETCD_MAX_SNAPSHOTS=&quot;5&quot;ETCD_MAX_WALS=&quot;5&quot;#ETCD_CORS=&quot;&quot;##[cluster]ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://192.168.7.11:2380&quot;# if you use different ETCD_NAME (e.g. test), set ETCD_INITIAL_CLUSTER value for this name, i.e. &quot;test=http://...&quot;ETCD_INITIAL_CLUSTER=&quot;etcd1=https://192.168.7.11:2380,etcd2=https://192.168.7.12:2380,etcd3=https://192.168.7.13:2380&quot;ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;ETCD_ADVERTISE_CLIENT_URLS=&quot;https://192.168.7.11:2379&quot;#ETCD_DISCOVERY=&quot;&quot;#ETCD_DISCOVERY_SRV=&quot;&quot;#ETCD_DISCOVERY_FALLBACK=&quot;proxy&quot;#ETCD_DISCOVERY_PROXY=&quot;&quot;#ETCD_STRICT_RECONFIG_CHECK=&quot;false&quot;#ETCD_AUTO_COMPACTION_RETENTION=&quot;0&quot;##[proxy]#ETCD_PROXY=&quot;off&quot;#ETCD_PROXY_FAILURE_WAIT=&quot;5000&quot;#ETCD_PROXY_REFRESH_INTERVAL=&quot;30000&quot;#ETCD_PROXY_DIAL_TIMEOUT=&quot;1000&quot;#ETCD_PROXY_WRITE_TIMEOUT=&quot;5000&quot;#ETCD_PROXY_READ_TIMEOUT=&quot;0&quot;##[security]ETCD_CERT_FILE=&quot;/etc/etcd/ssl/etcd.pem&quot;ETCD_KEY_FILE=&quot;/etc/etcd/ssl/etcd-key.pem&quot;ETCD_CLIENT_CERT_AUTH=&quot;true&quot;ETCD_TRUSTED_CA_FILE=&quot;/etc/etcd/ssl/etcd-root-ca.pem&quot;ETCD_AUTO_TLS=&quot;true&quot;ETCD_PEER_CERT_FILE=&quot;/etc/etcd/ssl/etcd.pem&quot;ETCD_PEER_KEY_FILE=&quot;/etc/etcd/ssl/etcd-key.pem&quot;ETCD_PEER_CLIENT_CERT_AUTH=&quot;true&quot;ETCD_PEER_TRUSTED_CA_FILE=&quot;/etc/etcd/ssl/etcd-root-ca.pem&quot;ETCD_PEER_AUTO_TLS=&quot;true&quot;##[logging]#ETCD_DEBUG=&quot;false&quot;# examples for -log-package-levels etcdserver=WARNING,security=DEBUG#ETCD_LOG_PACKAGE_LEVELS=&quot;&quot; 启动及验证 配置修改后在每个节点进行启动即可，注意，Etcd 各个节点间必须保证时钟同步，否则会造成启动失败等错误 1234567yum -y install ntpsystemctl start ntpdsystemctl enable ntpdhwclock --systohcsystemctl daemon-reloadsystemctl start etcdsystemctl enable etcd 验证 etcd 集群节点状态 12export ETCDCTL_API=3etcdctl --cacert=/etc/etcd/ssl/etcd-root-ca.pem --cert=/etc/etcd/ssl/etcd.pem --key=/etc/etcd/ssl/etcd-key.pem --endpoints=https://192.168.7.11:2379,https://192.168.7.12:2379,https://192.168.7.13:2379 endpoint health 部署 HA master由于 api server 会写入一些日志，所以先创建好相关目录，并做好授权，防止因为权限错误导致 api server 无法启动 12345for IP in `seq 1 3`;do ssh root@192.168.1.1$IP mkdir /var/log/kube-audit ssh root@192.168.1.1$IP chown -R kube:kube /var/log/kube-audit ssh root@192.168.1.1$IP chmod -R 755 /var/log/kube-auditdone 创建 kube-apiserver的service配置文件vim /usr/lib/systemd/system/kube-apiserver.service 1234567891011121314151617181920212223242526[Unit]Description=Kubernetes API ServiceDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.targetAfter=etcd.service[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/apiserverExecStart=/usr/local/bin/kube-apiserver \ $KUBE_LOGTOSTDERR \ $KUBE_LOG_LEVEL \ $KUBE_ETCD_SERVERS \ $KUBE_API_ADDRESS \ $KUBE_API_PORT \ $KUBELET_PORT \ $KUBE_ALLOW_PRIV \ $KUBE_SERVICE_ADDRESSES \ $KUBE_ADMISSION_CONTROL \ $KUBE_API_ARGSRestart=on-failureType=notifyLimitNOFILE=65536[Install]WantedBy=multi-user.target 创建 kube-controller-manager的serivce配置文件vim /usr/lib/systemd/system/kube-controller-manager.service 12345678910111213141516Description=Kubernetes Controller ManagerDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/controller-managerExecStart=/usr/local/bin/kube-controller-manager \ $KUBE_LOGTOSTDERR \ $KUBE_LOG_LEVEL \ $KUBE_MASTER \ $KUBE_CONTROLLER_MANAGER_ARGSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target 创建 kube-scheduler的serivce配置文件vim /usr/lib/systemd/system/kube-scheduler.service 1234567891011121314151617[Unit]Description=Kubernetes Scheduler PluginDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/schedulerExecStart=/usr/local/bin/kube-scheduler \ $KUBE_LOGTOSTDERR \ $KUBE_LOG_LEVEL \ $KUBE_MASTER \ $KUBE_SCHEDULER_ARGSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target 修改 master 配置master 需要编辑 config、apiserver、controller-manager、scheduler这四个文件，具体修改如下 config 通用配置vim /etc/kubernetes/config 12345678910111213141516171819202122#### kubernetes system config## The following values are used to configure various aspects of all# kubernetes services, including## kube-apiserver.service# kube-controller-manager.service# kube-scheduler.service# kubelet.service# kube-proxy.service# logging to stderr means we get it in the systemd journalKUBE_LOGTOSTDERR=&quot;--logtostderr=true&quot;# journal message level, 0 is debugKUBE_LOG_LEVEL=&quot;--v=2&quot;# Should this cluster be allowed to run privileged docker containersKUBE_ALLOW_PRIV=&quot;--allow-privileged=true&quot;# How the controller-manager, scheduler, and proxy find the apiserverKUBE_MASTER=&quot;--master=http://127.0.0.1:8080&quot; apiserver 配置(其他节点只有 IP 不同) vim /etc/kubernetes/apiserver 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#### kubernetes system config## The following values are used to configure the kube-apiserver## The address on the local server to listen to.KUBE_API_ADDRESS=&quot;--advertise-address=192.168.7.11 --insecure-bind-address=127.0.0.1 --bind-address=192.168.7.11&quot;# The port on the local server to listen on.KUBE_API_PORT=&quot;--insecure-port=8080 --secure-port=6443&quot;# Port minions listen on# KUBELET_PORT=&quot;--kubelet-port=10250&quot;# Comma separated list of nodes in the etcd clusterKUBE_ETCD_SERVERS=&quot;--etcd-servers=https://192.168.7.11:2379,https://192.168.7.12:2379,https://192.168.7.13:2379&quot;# Address range to use for servicesKUBE_SERVICE_ADDRESSES=&quot;--service-cluster-ip-range=10.254.0.0/16&quot;# default admission control policiesKUBE_ADMISSION_CONTROL=&quot;--admission-control=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota&quot;# Add your own!KUBE_API_ARGS=&quot;--authorization-mode=RBAC \ --runtime-config=rbac.authorization.k8s.io/v1beta1 \ --anonymous-auth=false \ --kubelet-https=true \ --experimental-bootstrap-token-auth \ --token-auth-file=/etc/kubernetes/token.csv \ --service-node-port-range=30000-50000 \ --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \ --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \ --client-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \ --service-account-key-file=/etc/kubernetes/ssl/k8s-root-ca.pem \ --etcd-quorum-read=true \ --storage-backend=etcd3 \ --etcd-cafile=/etc/etcd/ssl/etcd-root-ca.pem \ --etcd-certfile=/etc/etcd/ssl/etcd.pem \ --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \ --enable-swagger-ui=true \ --apiserver-count=3 \ --audit-log-maxage=30 \ --audit-log-maxbackup=3 \ --audit-log-maxsize=100 \ --audit-log-path=/var/log/kube-audit/audit.log \ --event-ttl=1h&quot; controller-manager 配置 vim /etc/kubernetes/controller-manager 1234567891011121314151617#### The following values are used to configure the kubernetes controller-manager# defaults from config and apiserver should be adequate# Add your own!KUBE_CONTROLLER_MANAGER_ARGS=&quot;--address=127.0.0.1 \ --service-cluster-ip-range=10.254.0.0/16 \ --cluster-name=kubernetes \ --cluster-signing-cert-file=/etc/kubernetes/ssl/k8s-root-ca.pem \ --cluster-signing-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \ --service-account-private-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \ --root-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \ --leader-elect=true \ --node-monitor-grace-period=40s \ --node-monitor-period=5s \ --pod-eviction-timeout=5m0s&quot; scheduler 配置 vim /etc/kubernetes/scheduler 1234567#### kubernetes scheduler config# default config should be adequate# Add your own!KUBE_SCHEDULER_ARGS=&quot;--leader-elect=true --address=127.0.0.1&quot; 其他 master 节点配置相同，只需要修改以下 IP 地址即可 启动各 master 节点相关服务1234567systemctl daemon-reloadsystemctl start kube-apiserversystemctl start kube-controller-managersystemctl start kube-schedulersystemctl enable kube-apiserversystemctl enable kube-controller-managersystemctl enable kube-scheduler 验证 master 节点功能 部署 node创建 ClusterRoleBindingkubelet 启动时向 kube-apiserver 发送 TLS bootstrapping 请求，需要先将 bootstrap token 文件中的 kubelet-bootstrap 用户赋予 system:node-bootstrapper cluster 角色(role)， 然后 kubelet 才能有权限创建认证请求(certificate signing requests)： 1234# 在任意 master 执行即可kubectl create clusterrolebinding kubelet-bootstrap \ --clusterrole=system:node-bootstrapper \ --user=kubelet-bootstrap 创建 kubelet 的service配置文件/usr/lib/systemd/system/kubelet.service 123456789101112131415161718192021222324[Unit]Description=Kubernetes Kubelet ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=docker.serviceRequires=docker.service[Service]WorkingDirectory=/var/lib/kubeletEnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/kubeletExecStart=/usr/local/bin/kubelet \ $KUBE_LOGTOSTDERR \ $KUBE_LOG_LEVEL \ $KUBELET_API_SERVER \ $KUBELET_ADDRESS \ $KUBELET_PORT \ $KUBELET_HOSTNAME \ $KUBE_ALLOW_PRIV \ $KUBELET_POD_INFRA_CONTAINER \ $KUBELET_ARGSRestart=on-failure[Install]WantedBy=multi-user.target 创建 kube-proxy 的service配置文件/usr/lib/systemd/system/kube-proxy.service 123456789101112131415161718[Unit]Description=Kubernetes Kube-Proxy ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.target[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/proxyExecStart=/usr/local/bin/kube-proxy \ $KUBE_LOGTOSTDERR \ $KUBE_LOG_LEVEL \ $KUBE_MASTER \ $KUBE_PROXY_ARGSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target config 通用配置 12345678910111213141516171819202122#### kubernetes system config## The following values are used to configure various aspects of all# kubernetes services, including## kube-apiserver.service# kube-controller-manager.service# kube-scheduler.service# kubelet.service# kube-proxy.service# logging to stderr means we get it in the systemd journalKUBE_LOGTOSTDERR=&quot;--logtostderr=true&quot;# journal message level, 0 is debugKUBE_LOG_LEVEL=&quot;--v=2&quot;# Should this cluster be allowed to run privileged docker containersKUBE_ALLOW_PRIV=&quot;--allow-privileged=true&quot;# How the controller-manager, scheduler, and proxy find the apiserver# KUBE_MASTER=&quot;--master=http://127.0.0.1:8080&quot; kubelet 配置 123456789101112131415161718192021222324252627#### kubernetes kubelet (minion) config# The address for the info server to serve on (set to 0.0.0.0 or &quot;&quot; for all interfaces)KUBELET_ADDRESS=&quot;--address=192.168.7.14&quot;# The port for the info server to serve on# KUBELET_PORT=&quot;--port=10250&quot;# You may leave this blank to use the actual hostnameKUBELET_HOSTNAME=&quot;--hostname-override=docker4.node&quot;# location of the api-server#KUBELET_API_SERVER=&quot;--api-servers=http://192.168.7.11:8080&quot;# Add your own!KUBELET_ARGS=&quot;--cgroup-driver=cgroupfs \ --cluster-dns=10.254.0.2 \ --resolv-conf=/etc/resolv.conf \ --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \ --require-kubeconfig \ --cert-dir=/etc/kubernetes/ssl \ --cluster-domain=cluster.local \ --hairpin-mode promiscuous-bridge \ --serialize-image-pulls=false \ --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.0&quot; proxy 配置 12345678910#### kubernetes proxy config# default config should be adequate# Add your own!KUBE_PROXY_ARGS=&quot;--bind-address=192.168.7.14 \ --hostname-override=docker4.node \ --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \ --cluster-cidr=10.254.0.0/16&quot; 创建 nginx 代理根据上面描述的 master HA 架构，此时所有 node 应该连接本地的 nginx 代理，然后 nginx 来负载所有 api server；以下为 nginx 代理相关配置 123456789101112131415161718192021222324252627282930313233# 创建配置目录mkdir -p /etc/nginx# 写入代理配置cat &lt;&lt; EOF &gt;&gt; /etc/nginx/nginx.conferror_log stderr notice;worker_processes auto;events &#123; multi_accept on; use epoll; worker_connections 1024;&#125;stream &#123; upstream kube_apiserver &#123; least_conn; server 192.168.7.11:6443; server 192.168.7.12:6443; server 192.168.7.13:6443; &#125; server &#123; listen 0.0.0.0:6443; proxy_pass kube_apiserver; proxy_timeout 10m; proxy_connect_timeout 1s; &#125;&#125;EOF# 更新权限chmod +r /etc/nginx/nginx.conf 为了保证 nginx 的可靠性，综合便捷性考虑，node 节点上的 nginx 使用 docker 启动，同时 使用 systemd 来守护， systemd 配置如下 12345678910111213141516171819202122232425cat &lt;&lt; EOF &gt;&gt; /etc/systemd/system/nginx-proxy.service[Unit]Description=kubernetes apiserver docker wrapperWants=docker.socketAfter=docker.service[Service]User=rootPermissionsStartOnly=trueExecStart=/usr/bin/docker run -p 127.0.0.1:6443:6443 \\ -v /etc/nginx:/etc/nginx \\ --name nginx-proxy \\ --net=host \\ --restart=on-failure:5 \\ --memory=512M \\ nginx:1.13.3-alpineExecStartPre=-/usr/bin/docker rm -f nginx-proxyExecStop=/usr/bin/docker stop nginx-proxyRestart=alwaysRestartSec=15sTimeoutStartSec=30s[Install]WantedBy=multi-user.targetEOF 启动 kubelet、nginx-proxy1234567systemctl daemon-reloadsystemctl start kubeletsystemctl enable kubeletsystemctl start kube-proxysystemctl enable kube-proxysystemctl start nginx-proxysystemctl enable nginx-proxy master 节点验证连通性1kubectl --server https://192.168.7.11:6443 --certificate-authority /etc/kubernetes/ssl/k8s-root-ca.pem --client-certificate /etc/kubernetes/ssl/admin.pem --client-key /etc/kubernetes/ssl/admin-key.pem get cs 12345678910111213# 查看 csr➜ kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-PhOGHB8BpFuNoYtnCGka4NTaxtDQDRjrctZtdaVsijY 2m kubelet-bootstrap Pending# 签发证书➜ kubectl certificate approve node-csr-PhOGHB8BpFuNoYtnCGka4NTaxtDQDRjrctZtdaVsijYcertificatesigningrequest &quot;node-csr-PhOGHB8BpFuNoYtnCGka4NTaxtDQDRjrctZtdaVsijY&quot; approved# 查看 node➜ kubectl get nodeNAME STATUS AGE VERSIONdocker4.node Ready 20s v1.7.4 安装 kubedns 插件安装 dashboard 插件安装 heapster 插件安装 EFK 插件部署 harbor 私有仓库]]></content>
      <categories>
        <category>devops</category>
        <category>kubernetes</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[悟透自己]]></title>
    <url>%2F2017%2F09%2F09%2F%E6%82%9F%E9%80%8F%E8%87%AA%E5%B7%B1%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;&nbsp;&nbsp;&nbsp;&nbsp;人生在世,和“自己”相处最多, 但是往往悟不透“自己”. &emsp;&emsp;&nbsp;&nbsp;&nbsp;&nbsp;人生走上坡路时,往往把自己估计过高,似乎一切所求的东西都能唾手可得,甚至把运气和机遇也看作自己身价的一部分而喜不自胜.人在不得意时,又往往把自己估计过低,把困难和不利也看做自己的无能,以至于把安分守己,与世无争误认为有自知之明,而实际上往往被怯懦的面具窒息了自己鲜活的生命. &emsp;&emsp;&nbsp;&nbsp;&nbsp;&nbsp;悟透自己,就是正确认识自己,也就是说要做一个冷静的现实主义者,既知道自己的优势,也知道自己的不足.我们可以憧憬人生,但期望值不能过高.因为在现实中,理想总是会打折扣的.可以迎接挑战.但是必须清楚自己努力的方向.也就是说,人一旦有了自知之明,也就没有什么克服不了的困难,没有什么过不去的难关. &emsp;&emsp;&nbsp;&nbsp;&nbsp;&nbsp;要悟透自己就要欣赏自己.无论你是一棵参天大树,还是一棵小草,无论你成为一座巍峨的高山,还是一快小小的石头,都是一种天然,都有自己存在的价值.只要你认真的欣赏自己,你就会拥有一个真正的自我.只有自我欣赏才会有信心,一旦拥有了信心也就会拥有了抵御一切逆境的动力. &emsp;&emsp;&nbsp;&nbsp;&nbsp;&nbsp;要悟透自己,就要心疼自己.在气愤时心疼一下自己,找个僻静处散散心,宣泄宣泄,不要让那些无名之火伤身;忧郁时,要心疼一下自己,找三五好友,诉说诉说,让感情的阴天变晴;劳累时,你要心疼一下自己,为自己来一番问寒问暖,要明白人所拥有的不过是一个血肉之躯,经不住太多的风力霜剑;有病时,你要心疼一下自己,惟有对自己的心疼,才是战胜疾病的信心和力量. &emsp;&emsp;&nbsp;&nbsp;&nbsp;&nbsp;悟透了自己,才能把握住自己,你生活才会有滋有味!]]></content>
      <categories>
        <category>ESSAYS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[JDK安装配置]]></title>
    <url>%2F2017%2F09%2F08%2FJDK%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[ubuntu1、解压文件并修改文件名 1sudo tar zxvf jdk-7u21-linux-i586.tar.gz -C /usr/lib/ 进入到安装目录中 1cd /usr/lib/ 修改文件名 1sudo mv jdk1.7.0_21 java 2、添加环境变量使用vim ~/.bashrc命令编辑 1234export JAVA_HOME=/usr/lib/javaexport JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/libexport PATH=$&#123;JAVA_HOME&#125;/bin:$PATH 3、激活环境变量 1source ~/.bashrc centos1、查看JDK信息 1rpm -qa | grep java 2、卸载OpenJDK——在 步骤 1中 复制所有的 openjdk卸载 1rpm -e --nodeps java-1.7.0-openjdk-headless-1.7.0.71-2.5.3.1.el7_0.x86_64 3、安装jdk 1sudo tar -zxvf yourjdk.tar.gz -C /usr/lib 4、配置JDK环境变量 1234export JAVA_HOME=/usr/lib/jdk1.8.0_92export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/binexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib 5、为了使编译生效 1source /etc/profile 6、查看path值 1echo $PATH Mac官网下载dmg版本安装后配置环境如下 1sudo vim ~/.bash_profile]]></content>
      <categories>
        <category>JAVA</category>
        <category>config</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F09%2F07%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
