<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[在CentOS7上部署kubernetes1.7 HA 集群(Vagrant)]]></title>
    <url>%2F2017%2F09%2F13%2F%E5%9C%A8CentOS7%E4%B8%8A%E9%83%A8%E7%BD%B2kubernetes1-7-HA-%E9%9B%86%E7%BE%A4-Vagrant%2F</url>
    <content type="text"><![CDATA[使用virtualbox + vagrant安装centos7vagrant准本工作1、具体安装和使用virtualbox &amp;&amp; vagrant请自行Google 2、搜索centos7 box:https://app.vagrantup.com/boxes/search?provider=virtualbox 3、生成Vagrantfile 123mkdir -p ~/Vagrant/k8s-clustercd ~/Vagrant/k8s-clustervagrant init centos/7 安装centos71、编辑Vagrantfile 123cd ~/Vagrant/k8s-clustercp Vagrantfile ./Vagrantfile_backupemacs Vagrantfile Vagrantfile 内容如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081# coding: utf-8Vagrant.configure(&quot;2&quot;) do |config| (11..13).each do |i| config.vm.define &quot;k8s-master#&#123;i&#125;&quot; do |node| # 设置虚拟机的Box node.vm.box = &quot;centos/7&quot; # 设置虚拟机的主机名 node.vm.hostname=&quot;master#&#123;i&#125;&quot; # 设置虚拟机的IP node.vm.network &quot;public_network&quot;, ip: &quot;192.168.7.#&#123;i&#125;&quot;, :bridge=&gt;&apos;en0: Wi-Fi (AirPort)&apos; node.vm.boot_timeout = 20 # 设置主机与虚拟机的共享目录 node.vm.synced_folder &quot;/Users/sunbjx/Vagrant/share&quot;, &quot;/home/vagrant/share&quot; # VirtaulBox相关配置 node.vm.provider &quot;virtualbox&quot; do |v| # 设置虚拟机的名称 v.name = &quot;k8s-master#&#123;i&#125;&quot; # 设置虚拟机的内存大小 v.memory = 4096 # 设置虚拟机的CPU个数 v.cpus = 2 end # 使用shell脚本进行软件安装和配置 node.vm.provision &quot;shell&quot;, inline: &lt;&lt;-SHELL sudo yum -y update sudo yum -y install net-tools sudo yum -y install vim SHELL end end (14..15).each do |i| config.vm.define &quot;k8s-node#&#123;i&#125;&quot; do |node| # 设置虚拟机的Box node.vm.box = &quot;centos/7&quot; # 设置虚拟机的主机名 node.vm.hostname=&quot;node#&#123;i&#125;&quot; # 设置虚拟机的IP node.vm.network &quot;public_network&quot;, ip: &quot;192.168.7.#&#123;i&#125;&quot;, :bridge=&gt;&apos;en0: Wi-Fi (AirPort)&apos; node.vm.boot_timeout = 20 # 设置主机与虚拟机的共享目录 node.vm.synced_folder &quot;/Users/sunbjx/Vagrant/share&quot;, &quot;/home/vagrant/share&quot; # VirtaulBox相关配置 node.vm.provider &quot;virtualbox&quot; do |v| # 设置虚拟机的名称 v.name = &quot;k8s-node#&#123;i&#125;&quot; # 设置虚拟机的内存大小 v.memory = 8192 # 设置虚拟机的CPU个数 v.cpus = 2 end # 使用shell脚本进行软件安装和配置 node.vm.provision &quot;shell&quot;, inline: &lt;&lt;-SHELL sudo yum -y update sudo yum -y install net-tools sudo yum -y install vim SHELL end endend 2、启动安装centos7 1vagrant up 环境说明环境总共 5 台虚拟机，3 个 master，3 个 etcd 节点，2 个 node 网络方案这里采用flannel，集群开启 RBAC IP 节点 192.168.7.11 master、etcd 192.168.7.12 master、etcd 192.168.7.13 master、etcd 192.168.7.14 node 192.168.7.15 node 创建 TLS证书和秘钥证书说明 证书名称 配置文件 用途 etcd-root-ca.pem etcd-root-ca-csr.json etcd 根 CA 证书 etcd.pem etcd-gencert.json、etcd-csr.json etcd 集群证书 k8s-root-ca.pem k8s-root-ca-csr.json k8s 根 CA 证书 kube-proxy.pem k8s-gencert.json、kube-proxy-csr.json kube-proxy 使用的证书 admin.pem k8s-gencert.json、admin-csr.json kubectl 使用的证书 kubernetes.pem k8s-gencert.json、kubernetes-csr.json kube-apiserver 使用的证书 安装CFSSL (192.168.7.11)12345678910111213mkdir -p ~/downloadcd ~/download# 下载 go1.8.3wget https://golang.org/dl/go1.8.3.linux-amd64.tar.gz# 解压压缩包tar -C /usr/local -xzf go1.8.3.linux-amd64.tar.gz# 添加环境变量vim /etc/profileexport GOROOT=/usr/local/goexport PATH=$PATH:$GOROOT/binsource /etc/profile# 使用 go 命令安装 cfsslgo get -u github.com/cloudflare/cfssl/cmd/... 生成etcd证书 (192.168.7.11) etcd 证书配置文件 12mkdir -p ~/document/sslcd ~/document/ssl vim etcd-root-ca-csr.json 12345678910111213141516&#123; &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 4096 &#125;, &quot;names&quot;: [ &#123; &quot;O&quot;: &quot;etcd&quot;, &quot;OU&quot;: &quot;etcd Security&quot;, &quot;L&quot;: &quot;Beijing&quot;, &quot;ST&quot;: &quot;Beijing&quot;, &quot;C&quot;: &quot;CN&quot; &#125; ], &quot;CN&quot;: &quot;etcd-root-ca&quot;&#125; vim etcd-gencert.json 12345678910111213&#123; &quot;signing&quot;: &#123; &quot;default&quot;: &#123; &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot; ], &quot;expiry&quot;: &quot;87600h&quot; &#125; &#125;&#125; vim etcd-csr.json 12345678910111213141516171819202122232425&#123; &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 4096 &#125;, &quot;names&quot;: [ &#123; &quot;O&quot;: &quot;etcd&quot;, &quot;OU&quot;: &quot;etcd Security&quot;, &quot;L&quot;: &quot;Beijing&quot;, &quot;ST&quot;: &quot;Beijing&quot;, &quot;C&quot;: &quot;CN&quot; &#125; ], &quot;CN&quot;: &quot;etcd&quot;, &quot;hosts&quot;: [ &quot;127.0.0.1&quot;, &quot;localhost&quot;, &quot;192.168.1.11&quot;, &quot;192.168.1.12&quot;, &quot;192.168.1.13&quot;, &quot;192.168.1.14&quot;, &quot;192.168.1.15&quot; ]&#125; “CN”：Common Name，kube-apiserver 从证书中提取该字段作为请求的用户名 (User Name)；浏览器使用该字段验证网站是否合法；“O”：Organization，kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group) 生成 etcd 证书 12cfssl gencert --initca=true etcd-root-ca-csr.json | cfssljson --bare etcd-root-cacfssl gencert --ca etcd-root-ca.pem --ca-key etcd-root-ca-key.pem --config etcd-gencert.json etcd-csr.json | cfssljson --bare etcd 生成 kubernetes 证书 (192.168.7.11) kubernetes 证书配置文件 vim k8s-root-ca-csr.json 12345678910111213141516&#123; &quot;CN&quot;: &quot;kubernetes&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 4096 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;BeiJing&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125; vim k8s-gencert.json 123456789101112131415161718&#123; &quot;signing&quot;: &#123; &quot;default&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot; &#125;, &quot;profiles&quot;: &#123; &quot;kubernetes&quot;: &#123; &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot; ], &quot;expiry&quot;: &quot;87600h&quot; &#125; &#125; &#125;&#125; vim kubernetes-csr.json 12345678910111213141516171819202122232425262728293031&#123; &quot;CN&quot;: &quot;kubernetes&quot;, &quot;hosts&quot;: [ &quot;127.0.0.1&quot;, &quot;10.254.0.1&quot;, &quot;192.168.7.11&quot;, &quot;192.168.7.12&quot;, &quot;192.168.7.13&quot;, &quot;192.168.7.14&quot;, &quot;192.168.7.15&quot;, &quot;localhost&quot;, &quot;kubernetes&quot;, &quot;kubernetes.default&quot;, &quot;kubernetes.default.svc&quot;, &quot;kubernetes.default.svc.cluster&quot;, &quot;kubernetes.default.svc.cluster.local&quot; ], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;BeiJing&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125; vim kube-proxy-csr.json 1234567891011121314151617&#123; &quot;CN&quot;: &quot;system:kube-proxy&quot;, &quot;hosts&quot;: [], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;BeiJing&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125; vim admin-csr.json 1234567891011121314151617&#123; &quot;CN&quot;: &quot;admin&quot;, &quot;hosts&quot;: [], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;BeiJing&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;O&quot;: &quot;system:masters&quot;, &quot;OU&quot;: &quot;System&quot; &#125; ]&#125; 生成 kubernetes 证书 12345cfssl gencert --initca=true k8s-root-ca-csr.json | cfssljson --bare k8s-root-cafor targetName in kubernetes admin kube-proxy; do cfssl gencert --ca k8s-root-ca.pem --ca-key k8s-root-ca-key.pem --config k8s-gencert.json --profile kubernetes $targetName-csr.json | cfssljson --bare $targetNamedone 分发所需证书到各个节点目录 /etc/kubernetes/ssl 下面 123456789for IP in `seq 1 5`;do ssh root@192.168.1.1$IP mkdir -p /etc/etcd/ssl sudo scp ~/document/ssl/etcd*.pem root@192.168.1.1$IP:/etc/etcd/ssl ssh root@192.168.1.1$IP chown -R etcd:etcd /etc/etcd/ssl ssh root@192.168.1.1$IP chmod -R 755 /etc/etcd ssh root@192.168.7.1$IP mkdir -p /etc/kubernetes/ssl sudo scp ~/document/ssl/*.pem root@192.168.7.1$IP:/etc/kubernetes/ssl ssh root@192.168.7.1$IP chown -R kube:kube /etc/kubernetes/ssldone 生成 token 及 kubeconfig (192.168.7.11)kubelet、kube-proxy 等 Node 机器上的进程与 Master 机器的 kube-apiserver 进程通信时需要认证和授权；kubernetes 1.4 开始支持由 kube-apiserver 为客户端生成 TLS 证书的 TLS Bootstrapping 功能，这样就不需要为每个客户端生成证书了；该功能当前仅支持为 kubelet 生成证书 生成 token 12345cd /etc/kubernetesexport BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d &apos; &apos;)cat &gt; token.csv &lt;&lt;EOF$&#123;BOOTSTRAP_TOKEN&#125;,kubelet-bootstrap,10001,&quot;system:kubelet-bootstrap&quot;EOF BOOTSTRAP_TOKEN 将被写入到 kube-apiserver 使用的 token.csv 文件和 kubelet 使用的 bootstrap.kubeconfig 文件，如果后续重新生成了 BOOTSTRAP_TOKEN，则需要：更新 token.csv 文件，分发到所有机器 (master 和 node）的 /etc/kubernetes/ 目录下，分发到node节点上非必需；重新生成 bootstrap.kubeconfig 文件，分发到所有 node 机器的 /etc/kubernetes/ 目录下；重启 kube-apiserver 和 kubelet 进程；重新 approve kubelet 的 csr 请求 安装 kubernetes 并将 kube-apiserver,kube-controller-manager,kube-scheduler,kubectl 分发到 master; kubelet,kube-proxy 分发到 node; 同时分发所需证书到各个节点目录 /etc/kubernetes/ssl 下面 123456789101112131415161718cd ~/downloadwget https://github.com/kubernetes/kubernetes/releases/download/v1.7.4/kubernetes.tar.gztar -zxvf kubernetes.tar.gzcd kubernetes./cluster/get-kube-binaries.shcd kubernetes/servertar -zxvf kubernetes-server-linux-amd64.tar.gzcd kubernetes/server/binfor IP in `seq 1 3`;do sudo scp ./&#123;kube-apiserver,kube-controller-manager,kube-scheduler,kubectl&#125; root@192.168.7.1$IP:/usr/local/bin/ ssh root@192.168.7.1$IP chmod a+x /usr/local/bin/kube*donefor IP in `seq 4 5`;do sudo scp ./&#123;kubelet,kube-proxy&#125; root@192.168.7.1$IP:/usr/local/bin ssh root@192.168.7.1$IP chmod a+x /usr/local/bin/kube*done 创建 kubelet bootstrapping kubeconfig 文件 12345678910111213141516171819cd /etc/kubernetesexport KUBE_APISERVER=&quot;https://192.168.7.11:6443&quot;# 设置集群参数kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/ssl/k8s-root-ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=bootstrap.kubeconfig# 设置客户端认证参数kubectl config set-credentials kubelet-bootstrap \ --token=$&#123;BOOTSTRAP_TOKEN&#125; \ --kubeconfig=bootstrap.kubeconfig# 设置上下文参数kubectl config set-context default \ --cluster=kubernetes \ --user=kubelet-bootstrap \ --kubeconfig=bootstrap.kubeconfig# 设置默认上下文kubectl config use-context default --kubeconfig=bootstrap.kubeconfig –embed-certs 为 true 时表示将 certificate-authority 证书写入到生成的 bootstrap.kubeconfig 文件中；设置客户端认证参数时没有指定秘钥和证书，后续由 kube-apiserver 自动生成 创建 kube-proxy kubeconfig 文件 12345678910111213141516171819# 设置集群参数kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/ssl/k8s-root-ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=kube-proxy.kubeconfig# 设置客户端认证参数kubectl config set-credentials kube-proxy \ --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \ --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \ --embed-certs=true \ --kubeconfig=kube-proxy.kubeconfig# 设置上下文参数kubectl config set-context default \ --cluster=kubernetes \ --user=kube-proxy \ --kubeconfig=kube-proxy.kubeconfig# 设置默认上下文kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig 设置集群参数和客户端认证参数时 –embed-certs 都为 true，这会将 certificate-authority、client-certificate 和 client-key 指向的证书文件内容写入到生成的 kube-proxy.kubeconfig 文件中；kube-proxy.pem 证书中 CN 为 system:kube-proxy，kube-apiserver 预定义的 RoleBinding cluster-admin 将User system:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限 分发 token、kubeconfig 文件 1234for IP in `seq 2 5`;do scp *.kubeconfig root@192.168.7.1$IP:/etc/kubernetes scp token.csv root@192.168.7.1$IP:/etc/kubernetesdone 部署 HA etcd安装 etcd123sudo yum -y install etcdecho &apos;export ETCDCTL_API=3&apos; &gt;&gt; /etc/profilesource /etc/profile 修改配置 vim /etc/etcd/etc.conf123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# [member]ETCD_NAME=etcd1ETCD_DATA_DIR=&quot;/var/lib/etcd/etcd1.etcd&quot;ETCD_WAL_DIR=&quot;/var/lib/etcd/wal&quot;ETCD_SNAPSHOT_COUNT=&quot;100&quot;ETCD_HEARTBEAT_INTERVAL=&quot;100&quot;ETCD_ELECTION_TIMEOUT=&quot;1000&quot;ETCD_LISTEN_PEER_URLS=&quot;https://192.168.7.11:2380&quot;ETCD_LISTEN_CLIENT_URLS=&quot;https://192.168.7.11:2379,http://127.0.0.1:2379&quot;ETCD_MAX_SNAPSHOTS=&quot;5&quot;ETCD_MAX_WALS=&quot;5&quot;#ETCD_CORS=&quot;&quot;##[cluster]ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://192.168.7.11:2380&quot;# if you use different ETCD_NAME (e.g. test), set ETCD_INITIAL_CLUSTER value for this name, i.e. &quot;test=http://...&quot;ETCD_INITIAL_CLUSTER=&quot;etcd1=https://192.168.7.11:2380,etcd2=https://192.168.7.12:2380,etcd3=https://192.168.7.13:2380&quot;ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;ETCD_ADVERTISE_CLIENT_URLS=&quot;https://192.168.7.11:2379&quot;#ETCD_DISCOVERY=&quot;&quot;#ETCD_DISCOVERY_SRV=&quot;&quot;#ETCD_DISCOVERY_FALLBACK=&quot;proxy&quot;#ETCD_DISCOVERY_PROXY=&quot;&quot;#ETCD_STRICT_RECONFIG_CHECK=&quot;false&quot;#ETCD_AUTO_COMPACTION_RETENTION=&quot;0&quot;##[proxy]#ETCD_PROXY=&quot;off&quot;#ETCD_PROXY_FAILURE_WAIT=&quot;5000&quot;#ETCD_PROXY_REFRESH_INTERVAL=&quot;30000&quot;#ETCD_PROXY_DIAL_TIMEOUT=&quot;1000&quot;#ETCD_PROXY_WRITE_TIMEOUT=&quot;5000&quot;#ETCD_PROXY_READ_TIMEOUT=&quot;0&quot;##[security]ETCD_CERT_FILE=&quot;/etc/etcd/ssl/etcd.pem&quot;ETCD_KEY_FILE=&quot;/etc/etcd/ssl/etcd-key.pem&quot;ETCD_CLIENT_CERT_AUTH=&quot;true&quot;ETCD_TRUSTED_CA_FILE=&quot;/etc/etcd/ssl/etcd-root-ca.pem&quot;ETCD_AUTO_TLS=&quot;true&quot;ETCD_PEER_CERT_FILE=&quot;/etc/etcd/ssl/etcd.pem&quot;ETCD_PEER_KEY_FILE=&quot;/etc/etcd/ssl/etcd-key.pem&quot;ETCD_PEER_CLIENT_CERT_AUTH=&quot;true&quot;ETCD_PEER_TRUSTED_CA_FILE=&quot;/etc/etcd/ssl/etcd-root-ca.pem&quot;ETCD_PEER_AUTO_TLS=&quot;true&quot;##[logging]#ETCD_DEBUG=&quot;false&quot;# examples for -log-package-levels etcdserver=WARNING,security=DEBUG#ETCD_LOG_PACKAGE_LEVELS=&quot;&quot; 启动及验证 配置修改后在每个节点进行启动即可，注意，Etcd 各个节点间必须保证时钟同步，否则会造成启动失败等错误 1234567yum -y install ntpsystemctl start ntpdsystemctl enable ntpdhwclock --systohcsystemctl daemon-reloadsystemctl start etcdsystemctl enable etcd 验证 etcd 集群节点状态 12export ETCDCTL_API=3etcdctl --cacert=/etc/etcd/ssl/etcd-root-ca.pem --cert=/etc/etcd/ssl/etcd.pem --key=/etc/etcd/ssl/etcd-key.pem --endpoints=https://192.168.7.11:2379,https://192.168.7.12:2379,https://192.168.7.13:2379 endpoint health 部署 HA master由于 api server 会写入一些日志，所以先创建好相关目录，并做好授权，防止因为权限错误导致 api server 无法启动 12345for IP in `seq 1 3`;do ssh root@192.168.1.1$IP mkdir /var/log/kube-audit ssh root@192.168.1.1$IP chown -R kube:kube /var/log/kube-audit ssh root@192.168.1.1$IP chmod -R 755 /var/log/kube-auditdone 创建 kube-apiserver的service配置文件vim /usr/lib/systemd/system/kube-apiserver.service 1234567891011121314151617181920212223242526[Unit]Description=Kubernetes API ServiceDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.targetAfter=etcd.service[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/apiserverExecStart=/usr/local/bin/kube-apiserver \ $KUBE_LOGTOSTDERR \ $KUBE_LOG_LEVEL \ $KUBE_ETCD_SERVERS \ $KUBE_API_ADDRESS \ $KUBE_API_PORT \ $KUBELET_PORT \ $KUBE_ALLOW_PRIV \ $KUBE_SERVICE_ADDRESSES \ $KUBE_ADMISSION_CONTROL \ $KUBE_API_ARGSRestart=on-failureType=notifyLimitNOFILE=65536[Install]WantedBy=multi-user.target 创建 kube-controller-manager的serivce配置文件vim /usr/lib/systemd/system/kube-controller-manager.service 12345678910111213141516Description=Kubernetes Controller ManagerDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/controller-managerExecStart=/usr/local/bin/kube-controller-manager \ $KUBE_LOGTOSTDERR \ $KUBE_LOG_LEVEL \ $KUBE_MASTER \ $KUBE_CONTROLLER_MANAGER_ARGSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target 创建 kube-scheduler的serivce配置文件vim /usr/lib/systemd/system/kube-scheduler.service 1234567891011121314151617[Unit]Description=Kubernetes Scheduler PluginDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/schedulerExecStart=/usr/local/bin/kube-scheduler \ $KUBE_LOGTOSTDERR \ $KUBE_LOG_LEVEL \ $KUBE_MASTER \ $KUBE_SCHEDULER_ARGSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target 修改 master 配置master 需要编辑 config、apiserver、controller-manager、scheduler这四个文件，具体修改如下 config 通用配置vim /etc/kubernetes/config 12345678910111213141516171819202122#### kubernetes system config## The following values are used to configure various aspects of all# kubernetes services, including## kube-apiserver.service# kube-controller-manager.service# kube-scheduler.service# kubelet.service# kube-proxy.service# logging to stderr means we get it in the systemd journalKUBE_LOGTOSTDERR=&quot;--logtostderr=true&quot;# journal message level, 0 is debugKUBE_LOG_LEVEL=&quot;--v=2&quot;# Should this cluster be allowed to run privileged docker containersKUBE_ALLOW_PRIV=&quot;--allow-privileged=true&quot;# How the controller-manager, scheduler, and proxy find the apiserverKUBE_MASTER=&quot;--master=http://127.0.0.1:8080&quot; apiserver 配置(其他节点只有 IP 不同) vim /etc/kubernetes/apiserver 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#### kubernetes system config## The following values are used to configure the kube-apiserver## The address on the local server to listen to.KUBE_API_ADDRESS=&quot;--advertise-address=192.168.7.11 --insecure-bind-address=127.0.0.1 --bind-address=192.168.7.11&quot;# The port on the local server to listen on.KUBE_API_PORT=&quot;--insecure-port=8080 --secure-port=6443&quot;# Port minions listen on# KUBELET_PORT=&quot;--kubelet-port=10250&quot;# Comma separated list of nodes in the etcd clusterKUBE_ETCD_SERVERS=&quot;--etcd-servers=https://192.168.7.11:2379,https://192.168.7.12:2379,https://192.168.7.13:2379&quot;# Address range to use for servicesKUBE_SERVICE_ADDRESSES=&quot;--service-cluster-ip-range=10.254.0.0/16&quot;# default admission control policiesKUBE_ADMISSION_CONTROL=&quot;--admission-control=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota&quot;# Add your own!KUBE_API_ARGS=&quot;--authorization-mode=RBAC \ --runtime-config=rbac.authorization.k8s.io/v1beta1 \ --anonymous-auth=false \ --kubelet-https=true \ --experimental-bootstrap-token-auth \ --token-auth-file=/etc/kubernetes/token.csv \ --service-node-port-range=30000-50000 \ --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \ --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \ --client-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \ --service-account-key-file=/etc/kubernetes/ssl/k8s-root-ca.pem \ --etcd-quorum-read=true \ --storage-backend=etcd3 \ --etcd-cafile=/etc/etcd/ssl/etcd-root-ca.pem \ --etcd-certfile=/etc/etcd/ssl/etcd.pem \ --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \ --enable-swagger-ui=true \ --apiserver-count=3 \ --audit-log-maxage=30 \ --audit-log-maxbackup=3 \ --audit-log-maxsize=100 \ --audit-log-path=/var/log/kube-audit/audit.log \ --event-ttl=1h&quot; controller-manager 配置 vim /etc/kubernetes/controller-manager 1234567891011121314151617#### The following values are used to configure the kubernetes controller-manager# defaults from config and apiserver should be adequate# Add your own!KUBE_CONTROLLER_MANAGER_ARGS=&quot;--address=127.0.0.1 \ --service-cluster-ip-range=10.254.0.0/16 \ --cluster-name=kubernetes \ --cluster-signing-cert-file=/etc/kubernetes/ssl/k8s-root-ca.pem \ --cluster-signing-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \ --service-account-private-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \ --root-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \ --leader-elect=true \ --node-monitor-grace-period=40s \ --node-monitor-period=5s \ --pod-eviction-timeout=5m0s&quot; scheduler 配置 vim /etc/kubernetes/scheduler 1234567#### kubernetes scheduler config# default config should be adequate# Add your own!KUBE_SCHEDULER_ARGS=&quot;--leader-elect=true --address=127.0.0.1&quot; 其他 master 节点配置相同，只需要修改以下 IP 地址即可 启动各 master 节点相关服务1234567systemctl daemon-reloadsystemctl start kube-apiserversystemctl start kube-controller-managersystemctl start kube-schedulersystemctl enable kube-apiserversystemctl enable kube-controller-managersystemctl enable kube-scheduler 验证 master 节点功能 部署 node创建 ClusterRoleBindingkubelet 启动时向 kube-apiserver 发送 TLS bootstrapping 请求，需要先将 bootstrap token 文件中的 kubelet-bootstrap 用户赋予 system:node-bootstrapper cluster 角色(role)， 然后 kubelet 才能有权限创建认证请求(certificate signing requests)： 1234# 在任意 master 执行即可kubectl create clusterrolebinding kubelet-bootstrap \ --clusterrole=system:node-bootstrapper \ --user=kubelet-bootstrap 创建 kubelet 的service配置文件/usr/lib/systemd/system/kubelet.service 123456789101112131415161718192021222324[Unit]Description=Kubernetes Kubelet ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=docker.serviceRequires=docker.service[Service]WorkingDirectory=/var/lib/kubeletEnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/kubeletExecStart=/usr/local/bin/kubelet \ $KUBE_LOGTOSTDERR \ $KUBE_LOG_LEVEL \ $KUBELET_API_SERVER \ $KUBELET_ADDRESS \ $KUBELET_PORT \ $KUBELET_HOSTNAME \ $KUBE_ALLOW_PRIV \ $KUBELET_POD_INFRA_CONTAINER \ $KUBELET_ARGSRestart=on-failure[Install]WantedBy=multi-user.target 创建 kube-proxy 的service配置文件/usr/lib/systemd/system/kube-proxy.service 123456789101112131415161718[Unit]Description=Kubernetes Kube-Proxy ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.target[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/proxyExecStart=/usr/local/bin/kube-proxy \ $KUBE_LOGTOSTDERR \ $KUBE_LOG_LEVEL \ $KUBE_MASTER \ $KUBE_PROXY_ARGSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target config 通用配置 12345678910111213141516171819202122#### kubernetes system config## The following values are used to configure various aspects of all# kubernetes services, including## kube-apiserver.service# kube-controller-manager.service# kube-scheduler.service# kubelet.service# kube-proxy.service# logging to stderr means we get it in the systemd journalKUBE_LOGTOSTDERR=&quot;--logtostderr=true&quot;# journal message level, 0 is debugKUBE_LOG_LEVEL=&quot;--v=2&quot;# Should this cluster be allowed to run privileged docker containersKUBE_ALLOW_PRIV=&quot;--allow-privileged=true&quot;# How the controller-manager, scheduler, and proxy find the apiserver# KUBE_MASTER=&quot;--master=http://127.0.0.1:8080&quot; kubelet 配置 123456789101112131415161718192021222324252627#### kubernetes kubelet (minion) config# The address for the info server to serve on (set to 0.0.0.0 or &quot;&quot; for all interfaces)KUBELET_ADDRESS=&quot;--address=192.168.7.14&quot;# The port for the info server to serve on# KUBELET_PORT=&quot;--port=10250&quot;# You may leave this blank to use the actual hostnameKUBELET_HOSTNAME=&quot;--hostname-override=docker4.node&quot;# location of the api-server#KUBELET_API_SERVER=&quot;--api-servers=http://192.168.7.11:8080&quot;# Add your own!KUBELET_ARGS=&quot;--cgroup-driver=cgroupfs \ --cluster-dns=10.254.0.2 \ --resolv-conf=/etc/resolv.conf \ --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \ --require-kubeconfig \ --cert-dir=/etc/kubernetes/ssl \ --cluster-domain=cluster.local \ --hairpin-mode promiscuous-bridge \ --serialize-image-pulls=false \ --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.0&quot; proxy 配置 12345678910#### kubernetes proxy config# default config should be adequate# Add your own!KUBE_PROXY_ARGS=&quot;--bind-address=192.168.7.14 \ --hostname-override=docker4.node \ --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \ --cluster-cidr=10.254.0.0/16&quot; 创建 nginx 代理根据上面描述的 master HA 架构，此时所有 node 应该连接本地的 nginx 代理，然后 nginx 来负载所有 api server；以下为 nginx 代理相关配置 123456789101112131415161718192021222324252627282930313233# 创建配置目录mkdir -p /etc/nginx# 写入代理配置cat &lt;&lt; EOF &gt;&gt; /etc/nginx/nginx.conferror_log stderr notice;worker_processes auto;events &#123; multi_accept on; use epoll; worker_connections 1024;&#125;stream &#123; upstream kube_apiserver &#123; least_conn; server 192.168.7.11:6443; server 192.168.7.12:6443; server 192.168.7.13:6443; &#125; server &#123; listen 0.0.0.0:6443; proxy_pass kube_apiserver; proxy_timeout 10m; proxy_connect_timeout 1s; &#125;&#125;EOF# 更新权限chmod +r /etc/nginx/nginx.conf 为了保证 nginx 的可靠性，综合便捷性考虑，node 节点上的 nginx 使用 docker 启动，同时 使用 systemd 来守护， systemd 配置如下 12345678910111213141516171819202122232425cat &lt;&lt; EOF &gt;&gt; /etc/systemd/system/nginx-proxy.service[Unit]Description=kubernetes apiserver docker wrapperWants=docker.socketAfter=docker.service[Service]User=rootPermissionsStartOnly=trueExecStart=/usr/bin/docker run -p 127.0.0.1:6443:6443 \\ -v /etc/nginx:/etc/nginx \\ --name nginx-proxy \\ --net=host \\ --restart=on-failure:5 \\ --memory=512M \\ nginx:1.13.3-alpineExecStartPre=-/usr/bin/docker rm -f nginx-proxyExecStop=/usr/bin/docker stop nginx-proxyRestart=alwaysRestartSec=15sTimeoutStartSec=30s[Install]WantedBy=multi-user.targetEOF 启动 kubelet、nginx-proxy1234567systemctl daemon-reloadsystemctl start kubeletsystemctl enable kubeletsystemctl start kube-proxysystemctl enable kube-proxysystemctl start nginx-proxysystemctl enable nginx-proxy master 节点验证连通性1kubectl --server https://192.168.7.11:6443 --certificate-authority /etc/kubernetes/ssl/k8s-root-ca.pem --client-certificate /etc/kubernetes/ssl/admin.pem --client-key /etc/kubernetes/ssl/admin-key.pem get cs 12345678910111213# 查看 csr➜ kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-PhOGHB8BpFuNoYtnCGka4NTaxtDQDRjrctZtdaVsijY 2m kubelet-bootstrap Pending# 签发证书➜ kubectl certificate approve node-csr-PhOGHB8BpFuNoYtnCGka4NTaxtDQDRjrctZtdaVsijYcertificatesigningrequest &quot;node-csr-PhOGHB8BpFuNoYtnCGka4NTaxtDQDRjrctZtdaVsijY&quot; approved# 查看 node➜ kubectl get nodeNAME STATUS AGE VERSIONdocker4.node Ready 20s v1.7.4 安装 kubedns 插件安装 dashboard 插件安装 heapster 插件安装 EFK 插件部署 harbor 私有仓库]]></content>
      <categories>
        <category>devops</category>
        <category>kubernetes</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[悟透自己]]></title>
    <url>%2F2017%2F09%2F09%2F%E6%82%9F%E9%80%8F%E8%87%AA%E5%B7%B1%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;&nbsp;&nbsp;&nbsp;&nbsp;人生在世,和“自己”相处最多, 但是往往悟不透“自己”. &emsp;&emsp;&nbsp;&nbsp;&nbsp;&nbsp;人生走上坡路时,往往把自己估计过高,似乎一切所求的东西都能唾手可得,甚至把运气和机遇也看作自己身价的一部分而喜不自胜.人在不得意时,又往往把自己估计过低,把困难和不利也看做自己的无能,以至于把安分守己,与世无争误认为有自知之明,而实际上往往被怯懦的面具窒息了自己鲜活的生命. &emsp;&emsp;&nbsp;&nbsp;&nbsp;&nbsp;悟透自己,就是正确认识自己,也就是说要做一个冷静的现实主义者,既知道自己的优势,也知道自己的不足.我们可以憧憬人生,但期望值不能过高.因为在现实中,理想总是会打折扣的.可以迎接挑战.但是必须清楚自己努力的方向.也就是说,人一旦有了自知之明,也就没有什么克服不了的困难,没有什么过不去的难关. &emsp;&emsp;&nbsp;&nbsp;&nbsp;&nbsp;要悟透自己就要欣赏自己.无论你是一棵参天大树,还是一棵小草,无论你成为一座巍峨的高山,还是一快小小的石头,都是一种天然,都有自己存在的价值.只要你认真的欣赏自己,你就会拥有一个真正的自我.只有自我欣赏才会有信心,一旦拥有了信心也就会拥有了抵御一切逆境的动力. &emsp;&emsp;&nbsp;&nbsp;&nbsp;&nbsp;要悟透自己,就要心疼自己.在气愤时心疼一下自己,找个僻静处散散心,宣泄宣泄,不要让那些无名之火伤身;忧郁时,要心疼一下自己,找三五好友,诉说诉说,让感情的阴天变晴;劳累时,你要心疼一下自己,为自己来一番问寒问暖,要明白人所拥有的不过是一个血肉之躯,经不住太多的风力霜剑;有病时,你要心疼一下自己,惟有对自己的心疼,才是战胜疾病的信心和力量. &emsp;&emsp;&nbsp;&nbsp;&nbsp;&nbsp;悟透了自己,才能把握住自己,你生活才会有滋有味!]]></content>
      <categories>
        <category>ESSAYS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[JDK安装配置]]></title>
    <url>%2F2017%2F09%2F08%2FJDK%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[ubuntu1、解压文件并修改文件名 1sudo tar zxvf jdk-7u21-linux-i586.tar.gz -C /usr/lib/ 进入到安装目录中 1cd /usr/lib/ 修改文件名 1sudo mv jdk1.7.0_21 java 2、添加环境变量使用vim ~/.bashrc命令编辑 1234export JAVA_HOME=/usr/lib/javaexport JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/libexport PATH=$&#123;JAVA_HOME&#125;/bin:$PATH 3、激活环境变量 1source ~/.bashrc centos1、查看JDK信息 1rpm -qa | grep java 2、卸载OpenJDK——在 步骤 1中 复制所有的 openjdk卸载 1rpm -e --nodeps java-1.7.0-openjdk-headless-1.7.0.71-2.5.3.1.el7_0.x86_64 3、安装jdk 1sudo tar -zxvf yourjdk.tar.gz -C /usr/lib 4、配置JDK环境变量 1234export JAVA_HOME=/usr/lib/jdk1.8.0_92export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/binexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib 5、为了使编译生效 1source /etc/profile 6、查看path值 1echo $PATH Mac官网下载dmg版本安装后配置环境如下 1sudo vim ~/.bash_profile]]></content>
      <categories>
        <category>JAVA</category>
        <category>config</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F09%2F07%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
